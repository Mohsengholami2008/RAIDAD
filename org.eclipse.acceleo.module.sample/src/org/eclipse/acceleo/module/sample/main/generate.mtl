[comment encoding = UTF-8 /]
[module generate('http://www.example.org/mLmodel')]
[import org::eclipse::acceleo::module::sample::main::helper /]
[import org::eclipse::acceleo::module::sample::main::IsClassOnline /]


[template public generateElement(aRoot : Root)]
[comment @main/]
[file (aRoot.name+'Model.py', false, 'UTF-8')]
[if (aRoot.process.cycle->filter(Import_Data)->size() > 0)]
import numpy as np
import warnings
np.warnings = warnings
import pandas as pd
[/if]
[if (aRoot.process.cycle->filter(Simple_Imputer)->size()>0)]
from sklearn.impute import SimpleImputer
[/if]
[if (aRoot.process.cycle->filter(Cross_Validation).plots->size()>0)]
import seaborn as sns
[/if]
[if (aRoot.process.cycle->filter(KNN_Imputer)->size()>0)]
from sklearn.impute import KNNImputer
[/if]
[if (aRoot.process.cycle->filter(Cross_Validation)->size()>0)]
from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc
from sklearn.model_selection import cross_validate, StratifiedKFold
[/if]
[if (aRoot.process.cycle->filter(Scaling)->size()>0)]
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
[/if]
[if (aRoot.process.cycle->filter(Date_to_Int)->size()>0)]
def extract_date_component_func(data, date_component_configs):
[/if]
[if (aRoot.process.cycle->filter(PCA)->size()>0)]
from sklearn.decomposition import PCA
[/if]
[if (aRoot.process.cycle->filter(KNN)->size()>0)]
from sklearn.neighbors import KNeighborsClassifier
[/if]
[if (aRoot.process.cycle->filter(SVM)->size()>0)]
from sklearn.svm import SVC
[/if]
[if (aRoot.process.cycle->filter(Random_Forest)->size()>0)]
from sklearn.ensemble import RandomForestClassifier
[/if]
[if ((aRoot.process.cycle->filter(MLP)->size()>0) and (aRoot.process.cycle->filter(MLP)->first().library = Neural_Libraries::scikit))]
from sklearn.neural_network import MLPClassifier
[/if]
[if (aRoot.process.cycle->filter(K_Means)->size()>0)]
from pyclustering.cluster.kmeans import kmeans
from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
[/if]
[if ((aRoot.process.cycle->filter(Cluster_Evaluation).scatter->size()>0) or (aRoot.process.cycle->filter(Cross_Validation).plots->size()>0))]
import matplotlib.pyplot as plt
[/if]
[if (aRoot.process.cycle->filter(DBSCAN)->size()>0)]
from pyclustering.cluster.dbscan import dbscan
[/if]
[if (aRoot.process.cycle->filter(K_Medians)->size()>0)]
from pyclustering.cluster.kmedians import kmedians
from pyclustering.cluster.center_initializer import random_center_initializer
[/if]
[if (aRoot.process.cycle->filter(OPtics)->size()>0)]
from pyclustering.cluster.optics import optics
[/if]
[if (aRoot.process.cycle->filter(Logistic_Regression)->size()>0)]
from sklearn.linear_model import LogisticRegression
[/if]

[comment Functions /]

[Read_Data(aRoot)/]
[Simple_Imputer(aRoot)/]
[Duplicates(aRoot)/]
[KNN_Imputer(aRoot)/]
[Scaling(aRoot)/]
[Date_to_Int(aRoot)/]
[Onehot(aRoot)/]
[PCA(aRoot)/]
[Auto_Prep(aRoot)/]

[if (aRoot.process.cycle->filter(Voting_Classifier)->size()=0)]
[KNN_Def(aRoot)/]
[SVM_Def(aRoot)/]
[Random_Forest_Def(aRoot)/]
[MLP_Def(aRoot)/]
[Logestic_Regression_Def(aRoot)/]
[/if]
[Voting_Def(aRoot)/]


[Cross_Validation(aRoot)/]

[K_means(aRoot)/]
[K_Medians_pyclustering_Def(aRoot)/]
[Optics_pyclustering_Def(aRoot)/]
[DBSCAN_pyclustering_Def(aRoot)/]





[comment Process Data /]
[Process_Data(aRoot)/]
[Send_Data(aRoot)/]



[/file]


[file (aRoot.name + 'Online.py', false, 'UTF-8')]


import json
import time
import csv
from pathlib import Path
from typing import Dict, Tuple, Optional, List, Union

[if (aRoot.process.cycle->filter(OnlineModelManager)->first().hisory = true)]
from collections import deque
[/if]


import numpy as np

[for (it : CommunicationProtocol | process.cycle->filter(CommunicationProtocol)->first())]
[if (it.mqtt->size() > 0)]
import paho.mqtt.client as mqtt
[/if]
[/for]

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
from sklearn.preprocessing import StandardScaler, RobustScaler
[/if]

[if (aRoot.process.cycle->filter(ARIMA)->isEmpty())]
from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Perceptron
[/if]

from sklearn.preprocessing import PolynomialFeatures

[if (aRoot.process.cycle->filter(ARIMA)->notEmpty())]
from statsmodels.tsa.arima.model import ARIMA
[/if]


[if (aRoot.process.cycle->filter(SGDClassifier)->size()>0)]
from sklearn.linear_model import SGDClassifier
[/if]

[if (aRoot.process.cycle->filter(PassiveAggressiveClassifier)->size()>0)]
from sklearn.linear_model import PassiveAggressiveClassifier
[/if]

[if (aRoot.process.cycle->filter(Perceptron)->size()>0)]
from sklearn.linear_model import Perceptron
[/if]

import logging


[for (it : CommunicationProtocol | process.cycle->filter(CommunicationProtocol)->first())]

[if (it.oclIsTypeOf(CommunicationProtocol))]

MQTT_BROKER = "[it.mqtt.MQTT_BROKER/]"
MQTT_PORT = [it.mqtt.MQTT_PORT/]
[/if]
[/for]


[for (ci : Topic | process.cycle->filter(CommunicationProtocol).mqtt.topic)]
[if (ci.oclIsTypeOf(Topic))]
MQTT_SENSOR_TOPIC = "[ci.MQTT_SENSOR_TOPIC/]"   
MQTT_CONTROL_TOPIC = "[ci.MQTT_CONTROL_TOPIC/]"  
[/if] 
[/for]


[if (process.cycle->filter(CommunicationProtocol).devices->notEmpty())]

REQUIRED_DEVICES = ['['/][for (ki : Devices | process.cycle->filter(CommunicationProtocol).devices) separator(', ')]"[ki.REQUIRED_DEVICES/]"[/for]]
[/if]

[if (process.cycle->filter(ARIMA)->notEmpty())]
TARGET_DEVICE = "[process.cycle->filter(CommunicationProtocol).role.oclAsType(Target).name/]"
[/if]

SLOT_SIZE_SECONDS = 5

TRAIN_ROW_LIMIT = 100

[if (process.cycle->filter(CommunicationProtocol).role.oclAsType(Target)->notEmpty() and process.cycle->filter(ARIMA)->isEmpty())]
CLASSES = np.array(['['/][for (ki : Target | process.cycle->filter(CommunicationProtocol).role.oclAsType(Target)) separator(', ')][ki.name/][/for]])
[/if]

METRICS_CSV_PATH = Path("metrics_log.csv")

METRICS_LOG_EVERY = 1  

SENSOR_VALUE_MIN = -1e3
SENSOR_VALUE_MAX = 1e6

ROBUST_BUFFER_MAX_ROWS = 500


[if (process.cycle->filter(OnlineModelManager)->first().hisory = true)]

N_LAGS = [process.cycle->filter(OnlineModelManager).N_LAGS/]
[/if]

MODEL_CONFIG = {

	"preprocess_steps": ['['/][for (c : Cycle | process.cycle->select(c | c.oclIsKindOf(Standardization) or c.oclIsKindOf(Normalization) or c.oclIsKindOf(FeatureDiffs))) separator(', ')][if (c.oclIsKindOf(Standardization))]"scale"[elseif (c.oclIsKindOf(Normalization))]"normalize"[elseif (c.oclIsKindOf(FeatureDiffs))]"diffs"[/if][/for][']'/],
    "model_type": [for (c : Cycle | process.cycle->select(c | c.oclIsKindOf(OnlineModelManager)))][if (c.oclIsKindOf(SGDClassifier))]"linear_svm"[elseif (c.oclIsKindOf(PassiveAggressiveClassifier))]"pa"[elseif (c.oclIsKindOf(LgSGDClassifier))]"logistic_sgd"[elseif (c.oclIsKindOf(Perceptron))]"perceptron"[elseif (c.oclIsKindOf(ARIMA))]"arima"[/if][/for],

[if (process.cycle->filter(ARIMA)->notEmpty())]	
    "arima_order": (2, 0, 2),
    "target_device": TARGET_DEVICE,
[/if]

[if (process.cycle->filter(OnlineAlert)->notEmpty())]	
    "alert_config": {
        "enabled": True,
        "target_label": 1,
        "trigger_on": "prediction",   
        "method": "mqtt",             
        "mqtt_topic": "alerts/label1",
        "disabled_labels": ['['/]0],       

        "phases": ['['/]"TEST"],           
    },
[/if]


}


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s ['['/]%(levelname)s] %(message)s",
)


[if (process.cycle->filter(CommunicationProtocol).buffer->notEmpty())]

class RowBuffer:


    def __init__(self, slot_id: int):
        self.slot_id = slot_id
        self.messages: Dict['['/]str, dict] = {}  # device_id -> data dict

    def add_message(self, device_id: str, data: dict) -> None:
        self.messages['['/]device_id] = data

    def is_complete(self) -> bool:
        return all(d in self.messages for d in REQUIRED_DEVICES)

    def build_row(self) -> Tuple['['/]np.ndarray, Optional['['/]int]]:

        values = ['['/]]
        label = None

        for dev in REQUIRED_DEVICES:
            msg = self.messages.get(dev)
            if msg is None:
                values.append(np.nan)
                continue

            val = msg.get("value")
            if val is None:
                values.append(np.nan)
            else:
                values.append(float(val))

            if label is None and "label" in msg and msg['['/]"label"] is not None:
                try:
                    label = int(msg['['/]"label"])
                except (TypeError, ValueError):
                    pass

        X_row = np.array(values, dtype=float)
        return X_row, label
[/if]




# =============================
# Preprocessor: pipeline of preprocessing stages
# =============================

class Preprocessor:
    """
    Preprocessing pipeline:
      Supported steps:
        - "clean"  : basic cleaning (NaN, clip, etc.)
[if (process.cycle->filter(FeatureDiffs)->notEmpty())]

        - "diffs"  : add pairwise feature differences
[/if]

[if (process.cycle->filter(Polynomial)->notEmpty())]
        - "poly2"  : PolynomialFeatures degree 2
[/if]

[if (process.cycle->filter(Min_Max)->notEmpty())]

        - "minmax" : online Min-Max scaling
[/if]
    """

    def __init__(self, steps: Union['['/]str, List['['/]str]]):
        if isinstance(steps, str):
            steps = ['['/]steps]
        self.steps: List['['/]str] = ['['/]s.lower() for s in steps]

[if (process.cycle->filter(Polynomial)->notEmpty())]
        self.poly: Optional['['/]PolynomialFeatures] = None
        self.poly_initialized = False
[/if]

[if (process.cycle->filter(Min_Max)->notEmpty())]

        self.minmax_initialized = False
        self.min_: Optional['['/]np.ndarray] = None
        self.max_: Optional['['/]np.ndarray] = None
[/if]


    def _clean_basic(self, X_row: np.ndarray) -> np.ndarray:
        X = X_row.astype(float).copy()

        if np.all(np.isnan(X)):
            X['['/]:] = 0.0
            return X

        nan_mask = np.isnan(X)
        if np.any(nan_mask):
            mean_val = np.nanmean(X)
            X['['/]nan_mask] = mean_val

        X = np.clip(X, SENSOR_VALUE_MIN, SENSOR_VALUE_MAX)
        return X



[if (process.cycle->filter(FeatureDiffs)->notEmpty())]
    def _add_diffs(self, X: np.ndarray) -> np.ndarray:
        n = X.shape['['/]0]
        if n < 2:
            return X

        diffs = ['['/]]
        for i in range(n):
            for j in range(i + 1, n):
                diffs.append(X['['/]i] - X['['/]j])

        diffs = np.array(diffs, dtype=float)
        return np.concatenate(['['/]X, diffs])
[/if]

[if (process.cycle->filter(Polynomial)->notEmpty())]

    def _apply_poly2(self, X: np.ndarray) -> np.ndarray:
        if X.ndim == 1:
            X_2d = X.reshape(1, -1)
        else:
            X_2d = X

        if not self.poly_initialized:
            self.poly = PolynomialFeatures(degree=2, include_bias=False)
            self.poly.fit(X_2d)
            self.poly_initialized = True

        X_poly = self.poly.transform(X_2d)['['/]0]
        return X_poly

[/if]


[if (process.cycle->filter(Min_Max)->notEmpty())]


    def _minmax_scale(self, X: np.ndarray) -> np.ndarray:
        X = X.astype(float)

        if not self.minmax_initialized:
            self.min_ = X.copy()
            self.max_ = X.copy()
            self.minmax_initialized = True
        else:
            self.min_ = np.minimum(self.min_, X)
            self.max_ = np.maximum(self.max_, X)

        denom = self.max_ - self.min_
        denom['['/]denom == 0] = 1.0

        return (X - self.min_) / denom
[/if]

    def transform(self, X_row: np.ndarray) -> np.ndarray:
        X = X_row.astype(float).copy()

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
        scaling_step_names = {
            "scale", "standard", "standardization", "standard_scaler",
            "normalize", "normalization", "l2norm", "l2_norm",
            "robust", "robust_scaling", "robust_scale", "robust_scaler",
        }

[/if]


                    
        for step in self.steps:
            s = step.lower()

            if s == "clean":
                X = self._clean_basic(X)
                continue    
[if (process.cycle->filter(FeatureDiffs)->notEmpty())]

            if s == "diffs":
                X = self._add_diffs(X)
                continue
[/if]

[if (process.cycle->filter(Polynomial)->notEmpty())]
            if s == "poly2":
                X = self._apply_poly2(X)
                continue
[/if]

[if (process.cycle->filter(Min_Max)->notEmpty())]

            if s == "minmax":
                X = self._minmax_scale(X)
                continue
[/if]


[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
            if s in scaling_step_names:
                logging.debug(
                    f"Step '{step}' ignored in Preprocessor "
                    f"(scaling handled in OnlineModelManager)."
                )
                continue
[/if]

            logging.warning(f"Unknown preprocessing step: {step}")

        return X
    

# =============================
# Build classifier from model type
# =============================

[if (process.cycle->filter(OnlineModelManager)->notEmpty() and process.cycle->filter(ARIMA)->isEmpty())]

def build_classifier(model_type: str):
    mt = model_type.lower()

[if (process.cycle->filter(LgSGDClassifier)->notEmpty())]
    if mt == "logistic_sgd":
        return SGDClassifier(
            loss="log_loss",
            learning_rate="[process.cycle->filter(LgSGDClassifier)->first().learning_rate/]",
            random_state=[process.cycle->filter(LgSGDClassifier)->first().random_state/]
        )
[/if]

[if (process.cycle->filter(SGDClassifier)->notEmpty())]

    if mt == "linear_svm":
        return SGDClassifier(
            loss="hinge",
            learning_rate="[process.cycle->filter(SGDClassifier)->first().learning_rate/]",
            random_state=[process.cycle->filter(SGDClassifier)->first().random_state/]
        )
[/if]

[if (process.cycle->filter(PassiveAggressiveClassifier)->notEmpty())]
    if mt == "pa":
        return PassiveAggressiveClassifier(random_state=[process.cycle->filter(PassiveAggressiveClassifier)->first().random_state/])
[/if]

[if (process.cycle->filter(Perceptron)->notEmpty())]
    if mt == "perceptron":
        return Perceptron(random_state=[process.cycle->filter(Perceptron)->first().random_state/])
[/if]


    else:
        logging.warning(f"Unknown MODEL_TYPE: {model_type}, using logistic_sgd.")
        return SGDClassifier(
            loss="log_loss",
            learning_rate="optimal",
            random_state=0
        )

[/if]


[if (process.cycle->filter(OnlineAlert)->notEmpty())]

# =============================
# AlertManager
# =============================

class AlertManager:

    def __init__(self, config: dict):
        self.enabled = config.get("enabled", False)
        self.target_label = config.get("target_label", 1)
        self.trigger_on = config.get("trigger_on", "prediction")
        self.method = config.get("method", "log")
        self.mqtt_topic = config.get("mqtt_topic", "alerts/label")

        self.disabled_labels = set(config.get("disabled_labels", ['['/]]))

        # ÙØ§Ø²Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¢Ù„Ø§Ø±Ù… Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ ÙØ¹Ø§Ù„ Ø§Ø³Øª (TRAIN / TEST)
        self.phases = set(config.get("phases", ['['/]"TRAIN", "TEST"]))

        self._mqtt_client = None  # Ø¨Ø±Ø§ÛŒ publish Ø¢Ù„Ø§Ø±Ù… Ø¨Ø§ MQTT

    def _ensure_mqtt_client(self):
        if self._mqtt_client is None:
            self._mqtt_client = mqtt.Client()
            self._mqtt_client.connect(MQTT_BROKER, MQTT_PORT, keepalive=60)

    def _send_log(self, message: str):
        logging.warning(f"['['/]ALERT] {message}")





    def _send_mqtt(self, payload: dict):
            try:
        # --- Convert numpy types to Python int ---
                if "slot_id" in payload:
                    payload['['/]"slot_id"] = int(payload['['/]"slot_id"])

                if "target_label" in payload:
                    payload['['/]"target_label"] = int(payload['['/]"target_label"])

                if payload.get("true_label") is not None:
                    payload['['/]"true_label"] = int(payload['['/]"true_label"])

                if payload.get("pred_label") is not None:
                    payload['['/]"pred_label"] = int(payload['['/]"pred_label"])

                if "timestamp" in payload:
                    payload['['/]"timestamp"] = float(payload['['/]"timestamp"])
        # -----------------------------------------

                self._ensure_mqtt_client()
                self._mqtt_client.publish(self.mqtt_topic, json.dumps(payload), qos=1)

        # ðŸ”¹ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ø¢Ù„Ø§Ø±Ù… Ø¯Ø± log:
                logging.info(f"['['/]ALERT/MQTT] topic={self.mqtt_topic} payload={payload}")

            except Exception as e:
                logging.exception(f"Failed to publish alert via MQTT: {e}")



    def _send_email(self, payload: dict):
        if not (self.email_to and self.email_from and self.smtp_host):
            logging.warning("Email alert configured but SMTP settings incomplete.")
            return

        try:
            import smtplib
            from email.mime.text import MIMEText

            body = json.dumps(payload, indent=2)
            msg = MIMEText(body)
            msg['['/]"Subject"] = f"IoT Alert: label={self.target_label}"
            msg['['/]"From"] = self.email_from
            msg['['/]"To"] = self.email_to

            with smtplib.SMTP(self.smtp_host, self.smtp_port) as server:
                server.starttls()
                server.send_message(msg)

        except Exception as e:
            logging.exception(f"Failed to send alert email: {e}")

    def maybe_alert(
        self,
        *,
        phase: str,
        slot_id: int,
        true_label: Optional['['/]int],
        pred_label: Optional['['/]int],
    ) -> None:
        if not self.enabled:
            return

        if phase.upper() not in self.phases:
            return

        if true_label in self.disabled_labels or pred_label in self.disabled_labels:
            return

        trigger = False
        if self.trigger_on == "prediction" and pred_label == self.target_label:
            trigger = True
        elif self.trigger_on == "true" and true_label == self.target_label:
            trigger = True
        elif self.trigger_on == "both" and (
            (true_label == self.target_label) or (pred_label == self.target_label)
        ):
            trigger = True

        if not trigger:
            return

        payload = {
            "timestamp": time.time(),
            "phase": phase,
            "slot_id": slot_id,
            "target_label": self.target_label,
            "true_label": true_label,
            "pred_label": pred_label,
        }

        if self.method == "log":
            self._send_log(json.dumps(payload))
        elif self.method == "mqtt":
            self._send_mqtt(payload)
        elif self.method == "email":
            self._send_email(payload)
        else:
            logging.warning(f"Unknown alert method: {self.method}")
[/if]

# =============================
# OnlineModelManager
# =============================

class OnlineModelManager:
    """
    Online model manager (Preprocessor + optional Scaling + Classifier) using partial_fit,
    manages TRAIN/TEST state and logs metrics.

    Scaling types detected from preprocess_steps:
      - Standardization: "standard", "scale", ...
      - Normalization:   "normalize", ...
      - Robust Scaling:  "robust", ...
    """
[if (process.cycle->filter(ARIMA)->isEmpty())]   
    def __init__(self, model_config: dict):
        preprocess_steps_cfg = model_config.get("preprocess_steps", ['['/]"clean"])
        model_type = model_config.get("model_type", "logistic_sgd")


[if (process.cycle->filter(OnlineAlert)->notEmpty())]
        alert_config = model_config.get("alert_config", {"enabled": False})
        self.alert_manager = AlertManager(alert_config)
[/if]

        if isinstance(preprocess_steps_cfg, str):
            preprocess_steps_cfg = ['['/]preprocess_steps_cfg]

        steps_lower = ['['/]s.lower() for s in preprocess_steps_cfg]

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]

        standard_names = {"scale", "standard", "standardization", "standard_scaler"}
        normalize_names = {"normalize", "normalization", "l2norm", "l2_norm"}
        robust_names = {"robust", "robust_scaling", "robust_scale", "robust_scaler"}





        self.scaler_type: Optional['['/]str] = None

        for s in steps_lower:
            if s in standard_names:
                if self.scaler_type is None:
                    self.scaler_type = "standard"
            elif s in normalize_names:
                if self.scaler_type is None:
                    self.scaler_type = "normalize"
            elif s in robust_names:
                if self.scaler_type is None:
                    self.scaler_type = "robust"

        scaling_step_names = standard_names | normalize_names | robust_names
        steps_for_preproc = ['['/]s for s in steps_lower if s not in scaling_step_names]
[else]
        steps_for_preproc = steps_lower  

[/if]
        self.preproc = Preprocessor(steps_for_preproc)

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
        self.scaler: Optional['['/]Union['['/]StandardScaler, RobustScaler]] = None
        self.scaler_initialized: bool = False

        self.robust_buffer: List['['/]np.ndarray] = ['['/]]
[/if]
        self.clf = build_classifier(model_type)
        self.model_initialized = False

        self.mode = "TRAIN"
        self.train_rows_seen = 0

        self.test_total = 0
        self.test_correct = 0

[if (process.cycle->filter(OnlineModelManager)->first().hisory = true)]
        self.ts_buffer: deque['['/]np.ndarray] = deque(maxlen=N_LAGS)
[/if]

        self._init_metrics_csv()






        logging.info(
            f"Initialized OnlineModelManager with MODEL_TYPE={model_type}, "
            f"PREPROCESS_STEPS={steps_for_preproc}[if aRoot.process.cycle->filter(Normalization)->notEmpty() or aRoot.process.cycle->filter(Standardization)->notEmpty() or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty() ], scaler_type={self.scaler_type}") [else]") [/if]

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
    @staticmethod
    def _l2_normalize(X_2d: np.ndarray) -> np.ndarray:
        norms = np.linalg.norm(X_2d, axis=1, keepdims=True)
        norms['['/]norms == 0.0] = 1.0
        return X_2d / norms

    def _scale_for_train(self, X_2d: np.ndarray) -> np.ndarray:
        if self.scaler_type == "standard":
            if self.scaler is None:
                self.scaler = StandardScaler()
            self.scaler.partial_fit(X_2d)
            self.scaler_initialized = True
            return self.scaler.transform(X_2d)

        elif self.scaler_type == "normalize":
            return self._l2_normalize(X_2d)

        elif self.scaler_type == "robust":
            self.robust_buffer.append(X_2d['['/]0].copy())
            if len(self.robust_buffer) >= ROBUST_BUFFER_MAX_ROWS:
                data = np.vstack(self.robust_buffer)
                if self.scaler is None:
                    self.scaler = RobustScaler()
                self.scaler.fit(data)
                self.scaler_initialized = True
                self.robust_buffer = self.robust_buffer['['/]-ROBUST_BUFFER_MAX_ROWS // 2:]

            if self.scaler is not None and self.scaler_initialized:
                return self.scaler.transform(X_2d)
            return X_2d

        return X_2d

    def _scale_for_test(self, X_2d: np.ndarray) -> np.ndarray:
        if self.scaler_type in ("standard", "robust"):
            if self.scaler is not None and self.scaler_initialized:
                return self.scaler.transform(X_2d)
            return X_2d

        elif self.scaler_type == "normalize":
            return self._l2_normalize(X_2d)

        return X_2d
[/if]

[/if]










[if aRoot.process.cycle->filter(ARIMA)->notEmpty()]
    def __init__(self, model_config: dict):
        preprocess_steps_cfg = model_config.get("preprocess_steps", ['['/]"clean"])
        if isinstance(preprocess_steps_cfg, str):
            preprocess_steps_cfg = ['['/]preprocess_steps_cfg]

        self.preproc = Preprocessor(preprocess_steps_cfg)

        # ARIMA parameters
        order = model_config.get("arima_order", (2, 0, 2))
        self.arima_order: Tuple['['/]int, int, int] = tuple(order)

        # Target device for ARIMA time-series
        self.target_device: str = model_config.get(
            "target_device",
            REQUIRED_DEVICES['['/]0]
        )

        if self.target_device not in REQUIRED_DEVICES:
            raise ValueError(
                f"TARGET_DEVICE={self.target_device} is not listed in REQUIRED_DEVICES!"
            )

        self.target_idx: int = REQUIRED_DEVICES.index(self.target_device)
        logging.info(
            f"ARIMA will use device '{self.target_device}' "
            f"at index {self.target_idx} as the time-series."
        )

        # Time-series data (list of floats)
        self.series: List['['/]float] = ['['/]]

        # ARIMA model and fit results
        self.model: Optional['['/]ARIMA] = None
        self.results = None

        # Mode: TRAIN / TEST
        self.mode = "TRAIN"
        self.train_rows_seen = 0

        # Metrics for prediction error in TEST mode
        self.test_count = 0
        self.abs_error_sum = 0.0

        # Refit ARIMA model after every N samples
        self.refit_every = 20

        self._init_metrics_csv()

        logging.info(
            f"Initialized Time-Series OnlineModelManager with ARIMA{self.arima_order}, "
            f"PREPROCESS_STEPS={preprocess_steps_cfg}"
        )
[/if]










    def _init_metrics_csv(self) -> None:
        if not METRICS_CSV_PATH.exists():
            with METRICS_CSV_PATH.open("w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(['['/]
                    "timestamp",
                    "phase",
                    "slot_id",
                    "row_index",
[if (process.cycle->filter(ARIMA)->isEmpty())]
	
                    "label",
                    "prediction",
                    "correct",
                    "cumulative_accuracy"
[/if]

[if (process.cycle->filter(ARIMA)->notEmpty())]
                    "value",          
                    "prediction",     
                    "abs_error",      
                    "mae_so_far"      

[/if]


                ])





[if (process.cycle->filter(ARIMA)->isEmpty())]

    def _append_metrics(
        self,
        phase: str,
        slot_id: int,
        row_index: int,
        label: Optional['['/]int],
        prediction: Optional['['/]int]
    ) -> None:
        correct_flag = None
        if label is not None and prediction is not None:
            self.test_total += 1
            if prediction == label:
                self.test_correct += 1
            correct_flag = int(prediction == label)
            acc = self.test_correct / self.test_total
        else:
            acc = self.test_correct / self.test_total if self.test_total > 0 else 0.0

        with METRICS_CSV_PATH.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(['['/]
                time.time(),
                phase,
                slot_id,
                row_index,
                label,
                prediction,
                correct_flag,
                acc
            ])
[/if]

[if (process.cycle->filter(ARIMA)->notEmpty())]
    def _append_metrics(
        self,
        phase: str,
        slot_id: int,
        row_index: int,
        value: Optional['['/]float],
        prediction: Optional['['/]float],
    ) -> None:
        abs_error = None
        mae = None

        if phase == "TEST" and value is not None and prediction is not None:
            self.test_count += 1
            err = abs(value - prediction)
            self.abs_error_sum += err
            abs_error = err
            mae = self.abs_error_sum / self.test_count

        with METRICS_CSV_PATH.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(['['/]
                time.time(),
                phase,
                slot_id,
                row_index,
                value,
                prediction,
                abs_error,
                mae
            ])

[/if]


[if (aRoot.process.cycle->filter(OnlineModelManager)->first().hisory = true)]
    # ---------- Time-series feature builder ----------

    def _build_timeseries_features(self, x_t: np.ndarray) -> Optional['['/]np.ndarray]:
        """
        Use past N_LAGS rows as features for current time step.
        Feature vector: ['['/]x_{t-1}, x_{t-2}, ..., x_{t-N_LAGS}]
        If we don't have enough history yet â†’ return None.
        """
        if len(self.ts_buffer) < N_LAGS:
            # store current row and wait until buffer filled
            self.ts_buffer.append(x_t)
            return None

        # we have N_LAGS past rows
        past = list(self.ts_buffer)['['/]-N_LAGS:]
        feat = np.concatenate(past)

        # update buffer: push current row for future steps
        self.ts_buffer.append(x_t)

        return feat

[/if]


[if (process.cycle->filter(ARIMA)->isEmpty())]
    def process_row(self, slot_id: int, X_row: np.ndarray, label: Optional['['/]int]) -> None:
[if (aRoot.process.cycle->filter(OnlineModelManager)->first().hisory = false)]
        X_pre = self.preproc.transform(X_row)
[/if]


[if (aRoot.process.cycle->filter(OnlineModelManager)->first().hisory = true)]
        x_pre = self.preproc.transform(X_row)

        # 2) convert to time-series feature
        ts_feat = self._build_timeseries_features(x_pre)
        if ts_feat is None:
            logging.info(
                f"['['/]Slot {slot_id}] Not enough history yet for time-series features "
                f"(need {N_LAGS} past rows)."
            )
            return
[/if]

        X_2d = [if (aRoot.process.cycle->filter(OnlineModelManager)->first().hisory = false)]X_pre[/if][if (aRoot.process.cycle->filter(OnlineModelManager)->first().hisory = true)]ts_feat[/if].reshape(1, -1)





        if self.mode == "TRAIN":
            self._train_step(slot_id, X_2d, label)
        else:
            self._test_step(slot_id, X_2d, label)
[/if]


[if (process.cycle->filter(ARIMA)->notEmpty())]
    def process_row(self, slot_id: int, X_row: np.ndarray, label: Optional['['/]int]) -> None:
        """
        For the ARIMA version, only a single sensor (target_device) is used as the time-series.
        The label is ignored.
        """

        # 1) Preprocess the entire row
        X_pre = self.preproc.transform(X_row.astype(float))

        # Ensure the target index is still valid after preprocessing
        if self.target_idx >= len(X_pre):
            logging.error(
                f"target_idx={self.target_idx} >= len(X_pre)={len(X_pre)}. "
                f"A preprocessing step may have altered the dimensions unexpectedly."
            )
            return

        # 2) Extract the target sensor value
        value = float(X_pre['['/]self.target_idx])

        if self.mode == "TRAIN":
            self._train_step(slot_id, value)
        else:
            self._test_step(slot_id, value)

    def _fit_arima_if_needed(self) -> None:
        """
        Fit or refit the ARIMA model if enough data is available.
        """
        if len(self.series) < max(self.arima_order['['/]0] + self.arima_order['['/]2], 5):
            # Not enough data points for ARIMA fitting
            return

        try:
            self.model = ARIMA(self.series, order=self.arima_order)
            self.results = self.model.fit()
            logging.info(f"ARIMA{self.arima_order} fitted on {len(self.series)} points.")
        except Exception as e:
            logging.exception(f"Error fitting ARIMA model: {e}")
            self.model = None
            self.results = None

    def _train_step(self, slot_id: int, value: float) -> None:
        # Append new value to time-series
        self.series.append(value)
        self.train_rows_seen += 1

        logging.info(
            f"['['/]TRAIN] row #{self.train_rows_seen} | Slot={slot_id} | "
            f"{self.target_device}_value={value}"
        )

        # Refit the model every N rows
        if (self.train_rows_seen % self.refit_every) == 0:
            self._fit_arima_if_needed()

        # Log metrics for TRAIN phase (no prediction)
        if self.train_rows_seen % METRICS_LOG_EVERY == 0:
            self._append_metrics(
                phase="TRAIN",
                slot_id=slot_id,
                row_index=self.train_rows_seen,
                value=value,
                prediction=None,
            )

        # Automatically switch to TEST mode after TRAIN_ROW_LIMIT
        if self.train_rows_seen >= TRAIN_ROW_LIMIT:
            # Final fit before entering TEST mode
            self._fit_arima_if_needed()
            self.mode = "TEST"
            logging.info(
                f"TRAIN phase finished after {self.train_rows_seen} rows. "
                f"Switched to TEST mode (auto)."
            )
[/if]



[if (process.cycle->filter(ARIMA)->isEmpty())]
    def _train_step(self, slot_id: int, X_2d: np.ndarray, label: Optional['['/]int]) -> None:
        if label is None:
            logging.warning("In TRAIN mode but no label provided; only scaling warm-up.")

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
            X_in = self._scale_for_train(X_2d)

[else]
            X_in = X_2d
[/if]
            if self.model_initialized:
                y_pred = self.clf.predict(X_in)['['/]0]
                logging.info(f"['['/]TRAIN/no-label] Slot={slot_id} Pred={y_pred}")
            return

[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
        X_in = self._scale_for_train(X_2d)
[else]
        X_in = X_2d

[/if]
        y_arr = np.array(['['/]label])
        if not self.model_initialized:
            self.clf.partial_fit(X_in, y_arr, classes=CLASSES)
            self.model_initialized = True
            logging.info("Model initialized with first partial_fit.")
        else:
            self.clf.partial_fit(X_in, y_arr)

        y_pred = self.clf.predict(X_in)['['/]0]
        self.train_rows_seen += 1
        logging.info(
            f"['['/]TRAIN] row #{self.train_rows_seen} | Slot={slot_id} "
            f"true={label}, pred={y_pred}"
        )


[if (process.cycle->filter(OnlineAlert)->notEmpty())]

        self.alert_manager.maybe_alert(
            phase="TRAIN",
            slot_id=slot_id,
            true_label=label,
            pred_label=y_pred,
        )
[/if]

        if self.train_rows_seen % METRICS_LOG_EVERY == 0:
            self._append_metrics(
                phase="TRAIN",
                slot_id=slot_id,
                row_index=self.train_rows_seen,
                label=label,
                prediction=y_pred,
            )

        if self.train_rows_seen >= TRAIN_ROW_LIMIT:
            self.mode = "TEST"
            logging.info(
                f"TRAIN phase finished after {self.train_rows_seen} rows. "
                f"Switched to TEST mode (auto)."
            )
[/if]
[if (process.cycle->filter(ARIMA)->isEmpty())]

    def _test_step(self, slot_id: int, X_2d: np.ndarray, label: Optional['['/]int]) -> None:
        if not self.model_initialized:
            logging.warning("Model not initialized; cannot predict.")
            return
[if aRoot.process.cycle->filter(Normalization)->notEmpty()
   or aRoot.process.cycle->filter(Standardization)->notEmpty()
   or aRoot.process.cycle->filter(Robust_Scaling)->notEmpty()
]
        X_in = self._scale_for_test(X_2d)
[else]
        X_in = X_2d

[/if]
        y_pred = self.clf.predict(X_in)['['/]0]

        logging.info(f"['['/]TEST] Slot={slot_id} pred={y_pred}, true_label={label}")


[if (process.cycle->filter(OnlineAlert)->notEmpty())]

        self.alert_manager.maybe_alert(
            phase="TEST",
            slot_id=slot_id,
            true_label=label,
            pred_label=y_pred,
        )
[/if]


        self._append_metrics(
            phase="TEST",
            slot_id=slot_id,
            row_index=self.train_rows_seen + self.test_total + 1,
            label=label,
            prediction=y_pred,
        )


[/if]




[if (process.cycle->filter(ARIMA)->notEmpty())]

    def _test_step(self, slot_id: int, value: float) -> None:
        """
        In TEST mode:
          - If a model exists: predict one step ahead.
          - Log the prediction error.
          - Append the actual value to the series and periodically refit.
        """
        prediction = None

        if self.results is not None:
            try:
                # One-step forecast
                prediction = float(self.results.forecast(steps=1)['['/]0])
                logging.info(
                    f"['['/]TEST] Slot={slot_id} | {self.target_device}_predicted={prediction:.4f} "
                    f"| actual={value:.4f}"
                )
            except Exception as e:
                logging.exception(f"Error in ARIMA forecast: {e}")
                prediction = None
        else:
            logging.warning("ARIMA model is not fitted yet; cannot forecast.")

        # Metrics (MAE, etc.)
        row_index = self.train_rows_seen + self.test_count + 1
        self._append_metrics(
            phase="TEST",
            slot_id=slot_id,
            row_index=row_index,
            value=value,
            prediction=prediction,
        )

        # Append actual value to the series
        self.series.append(value)

        # Periodically refit the model
        if (len(self.series) % self.refit_every) == 0:
            self._fit_arima_if_needed()

[/if]




    def set_mode(self, new_mode: str, reason: str = "") -> None:
        new_mode = new_mode.upper()
        if new_mode not in ("TRAIN", "TEST"):
            logging.warning(f"Invalid mode request: {new_mode}")
            return
        self.mode = new_mode
        logging.info(f"Mode changed to {self.mode} ({reason})")

[if (process.cycle->filter(ARIMA)->notEmpty())]





        if self.mode == "TEST" and self.results is None:
            self._fit_arima_if_needed()

[/if]

# =============================
# Slot management
# =============================

slots: Dict['['/]int, RowBuffer] = {}

def compute_slot_id(timestamp: float) -> int:
    return int(timestamp // SLOT_SIZE_SECONDS)

def get_or_create_row_buffer(slot_id: int) -> RowBuffer:
    if slot_id not in slots:
        slots['['/]slot_id] = RowBuffer(slot_id)
    return slots['['/]slot_id]

def remove_old_slots(current_slot_id: int, max_age_slots: int = 10) -> None:
    to_delete = ['['/]sid for sid in slots if sid < current_slot_id - max_age_slots]
    for sid in to_delete:
        del slots['['/]sid]

# =============================
# MQTT callbacks
# =============================

model_manager = OnlineModelManager(MODEL_CONFIG)

def on_connect(client, userdata, flags, rc):
    if rc == 0:
        logging.info("Connected to MQTT broker.")
    else:
        logging.error(f"Failed to connect to MQTT broker. rc={rc}")

    client.subscribe(MQTT_SENSOR_TOPIC)
    logging.info(f"Subscribed to sensors topic: {MQTT_SENSOR_TOPIC}")

    client.subscribe(MQTT_CONTROL_TOPIC)
    logging.info(f"Subscribed to control topic: {MQTT_CONTROL_TOPIC}")

def on_message(client, userdata, msg):
    try:
        payload = msg.payload.decode("utf-8")

        if msg.topic == MQTT_CONTROL_TOPIC:
            mode_cmd = payload.strip().upper()
            if mode_cmd in ("TRAIN", "TEST"):
                model_manager.set_mode(mode_cmd, reason="by MQTT control/mode")
            else:
                logging.warning(f"Unknown control command on {MQTT_CONTROL_TOPIC}: {payload}")
            return

        data = json.loads(payload)

        device_id = data.get("device_id")
        if device_id is None:
            logging.warning("Received message without device_id; ignoring.")
            return

        ts = float(data.get("timestamp", time.time()))
        slot_id = compute_slot_id(ts)

        row_buffer = get_or_create_row_buffer(slot_id)
        row_buffer.add_message(device_id, data)

        remove_old_slots(slot_id)

        if row_buffer.is_complete():
            X_row, label = row_buffer.build_row()
            logging.info(
                f"['['/]Slot {slot_id}] NEW ROW | mode={model_manager.mode} | "
                f"X={X_row}, label={label}"
            )

            model_manager.process_row(slot_id, X_row, label)

            if slot_id in slots:
                del slots['['/]slot_id]

    except Exception as e:
        logging.exception(f"Error in on_message: {e}")

# =============================
# main
# =============================

def main():
    client = mqtt.Client()
    client.on_connect = on_connect
    client.on_message = on_message

    logging.info("Connecting to MQTT broker...")
    client.connect(MQTT_BROKER, MQTT_PORT, keepalive=60)

    logging.info("MQTT client started. Waiting for messages...")
    client.loop_forever()

if __name__ == "__main__":
    main()


[/file]


[file (aRoot.name + 'DataUnders', false, 'UTF-8')]
hi
[/file]
[/template]

[template public Read_Data(root : Root)]
[for (it : Cycle | root.process.cycle)]
	[comment read csv /]
[if (it.oclIsTypeOf(CSV))]
def read_csv(file_path):
    data = pd.read_csv(file_path, [for (by : CSV_Argumans | it.oclAsType(CSV).csv_argumans) separator (', ')][if (by.oclIsTypeOf(HeadCSV))]header=[by.oclAsType(HeadCSV).default_header/][/if][if (by.oclIsTypeOf(Sep) and by.oclAsType(Sep).manual_sep = false)]sep='[by.oclAsType(Sep).default_value_sep/]'[/if][if (by.oclIsTypeOf(Sep) and by.oclAsType(Sep).manual_sep = true)]sep='[by.oclAsType(Sep).define/]'[/if][if (by.oclIsTypeOf(Nrows))]nrows=[by.oclAsType(Nrows).default_value_nrows/][/if][/for])
    return data
[/if]

[if (it.oclIsTypeOf(Excel))]
def read_excel(file_path):
    data = pd.read_excel(file_path, [for (by : Excel_Argumans | it.oclAsType(Excel).excel_arguments) separator (', ')][if (by.oclIsTypeOf(Sheet_Na_Excel))]sheet_name='[by.oclAsType(Sheet_Na_Excel).default_val_sheet/]'[/if][if (by.oclIsTypeOf(Header_Ex))]header=[by.oclAsType(Header_Ex).defauld_value_head_ex/][/if][/for])
    return data
[/if]
[/for]
[/template]


[template public Date_to_Int(root : Root)]

[for (it : Cycle | process.cycle->filter(Date_to_Int)->first())]
    for date_component_config in date_component_configs:
        date_column_name = date_component_config.get('date_column_name')
        date_components = date_component_config.get('date_components')
        date_components_format = date_component_config.get('formtat')


        if not all(component in ['['/]'year', 'month', 'day'] for component in date_components):
            raise ValueError("Invalid date component. Use 'year', 'month', or 'day'.")

        if date_column_name not in data.columns:
            raise ValueError(f"The specified date column '{date_column_name}' does not exist in the dataset.")

        data['['/]date_column_name] = pd.to_datetime(data['['/]date_column_name], format='%d/%m/%Y')

        for date_component in date_components:
            if date_component == 'year':
                data['['/]f'{date_column_name}_{date_component}'] = data['['/]date_column_name].dt.year
            elif date_component == 'month':
                data['['/]f'{date_column_name}_{date_component}'] = data['['/]date_column_name].dt.month
            elif date_component == 'day':
                data['['/]f'{date_column_name}_{date_component}'] = data['['/]date_column_name].dt.day

        data = data.drop(columns=['['/]date_column_name])

    return data
[/for]
[/template]


[template public PCA(root : Root)]

[for (it : Cycle | process.cycle->filter(PCA))]
	[if (it.oclIsTypeOf(PCA))]

def apply_pca(data, n_components, target_column = None):
    if target_column and target_column in data.columns:

        features = data.drop(['['/]target_column], axis=1)
    else:
        features = data
        
    pca = PCA(n_components=n_components)
    principal_components = pca.fit_transform(features)
    n_pcs = pca.n_components_
    column_names = ['['/]f"PC{i+1}" for i in range(n_pcs)]
    pca_df = pd.DataFrame(principal_components, columns=column_names)
[if (root.process.cycle->filter(Classification)->size() > 0)]
    if target_column and target_column in data.columns:
        pca_df['['/]target_column] = data['['/]target_column].values
[/if]
    
    return pca_df
	[/if]
[/for]
[/template]

[template public Simple_Imputer(root : Root)]
[if (root.process.cycle->filter(Simple_Imputer)->size()>0)]

import logging
[for (it : Cycle | process.cycle->filter(Simple_Imputer)->first())]
def fill_missing_values(data, impute_configurations):
    if not impute_configurations:
        raise ValueError("No imputation configurations provided")
    
    for config in impute_configurations:
        column_names = config.get('column_names')
        if not all(col in data.columns for col in column_names):
            logger.error(f"One or more columns not found in the data: {column_names}")
            raise KeyError(f"One or more columns missing from data: {column_names}")
            
        try:
            imputer = SimpleImputer(
                missing_values=config.get('missing_values', np.nan),
                strategy=config.get('strategy', 'mean'),
                fill_value=config.get('fill_value')
            )
            data['['/]column_names] = imputer.fit_transform(data['['/]column_names])
            logger.info(f"Imputed missing values in columns: {column_names}")
        except Exception as e:
            logger.error(f"An error occurred during imputation: {e}")
            raise
    return data
[/for]
logger = logging.getLogger(__name__)
[/if]
[/template]

[template public Duplicates(root : Root)]
[for (it : Cycle | process.cycle->filter(Duplicates)->first())]

def remove_duplicates(data, remove_duplicates_columns=None):
    if remove_duplicates_columns is None:
        data = data.drop_duplicates()
    else:
        subset = remove_duplicates_columns.get('subset', None)
        keep = remove_duplicates_columns.get('keep', 'first')
        data = data.drop_duplicates(subset=subset, keep=keep)

    return data
[/for]
[/template]

[template public Scaling(root : Root)?( (root.process.cycle->filter(CSV).role->filter(Target)->size() = 0))]
[for (it : Cycle | process.cycle->filter(Scaling)->first())]

def scale_features(data, scaling_config):
    scaled_data = data.copy()  

    for method, columns in scaling_config.items():
        scaler = None
        if columns:  
            if method == "standardization":
                scaler = StandardScaler()
            elif method == "minmax":
                scaler = MinMaxScaler()
            elif method == "robust":
                scaler = RobustScaler()
            if scaler:
                scaler.fit(data['['/]columns])
                scaled_columns = scaler.transform(data['['/]columns])
                scaled_data['['/]columns] = scaled_columns

    return scaled_data
[/for]

[/template]


[template public Scaling(root : Root)?((root.process.cycle->filter(CSV).role->filter(Target)->size()>0))]
[for (it : Cycle | process.cycle->filter(Scaling)->first())]
def scale_features(data, target_column, scaling_config):
    features_to_exclude = ['['/]target_column]
    features = data.drop(columns=features_to_exclude, axis=1, errors='ignore')
    scaled_data = data['['/]features_to_exclude].copy()  

    for method, columns in scaling_config.items():
        scaler = None
        if columns:  
            if method == "standardization":
                scaler = StandardScaler()
            elif method == "minmax":
                scaler = MinMaxScaler()
            elif method == "robust":
                scaler = RobustScaler()
            if scaler:
                scaler.fit(data['['/]columns])
                scaled_columns = scaler.transform(data['['/]columns])
                scaled_data = scaled_data.join(pd.DataFrame(scaled_columns, columns=columns, index=data.index))

    for col in data.columns:
        if col not in scaled_data.columns:
            scaled_data['['/]col] = data['['/]col]
    
    return scaled_data
[/for]
[/template]

[template public Onehot(root : Root)]
[for (it : Cycle | process.cycle->filter(OneHot)->first())]
[if (it.oclIsTypeOf(OneHot))]
def one_hot_encoding(data, columns):
    return pd.get_dummies(data, columns=columns)
[/if]
[/for]
[/template]

[template public KNN_Imputer(root : Root)]
[for (it : KNN_Imputer | process.cycle->filter(KNN_Imputer))]
def knn_impute_missing_values(data, column_names, target_column=None,[for (by : KNN_Argumans | it.knn_argumans) separator (', ')][if (by.oclIsTypeOf(N_Neighbors))]n_neighbors=5[/if][if (by.oclIsTypeOf(Weights))]weights='uniform'[/if][if (by.oclIsTypeOf(Metric))]metric='nan_euclidean'[/if][/for]):
	if target_column:
		target = data['['/]target_column]
		data = data.drop(target_column, axis=1)
    
	knn_imputer = KNNImputer([for (by : KNN_Argumans | it.knn_argumans) separator (', ')][if (by.oclIsTypeOf(N_Neighbors))]n_neighbors=5[/if][if (by.oclIsTypeOf(Weights))]weights='uniform'[/if][if (by.oclIsTypeOf(Metric))]metric='nan_euclidean'[/if][/for])
	data['['/]column_names] = knn_imputer.fit_transform(data['['/]column_names])
    
	if target_column:
		data['['/]target_column] = target
    
	logger.info(f"Imputed missing values in columns: {column_names} using KNN imputer with {n_neighbors} neighbors, {weights} weights, and {metric} metric.")
	return data
[/for]
[/template]

[template public KNN_Def(root : Root) ? (root.process.cycle->filter(KNN)->size() > 0 and root.process.cycle->filter(Cross_Validation)->size() > 0)]
def build_KNN(KNN_params):
    KNN_model = KNeighborsClassifier(n_neighbors=KNN_params['['/]'k'])
    return KNN_model
[/template]

[template public SVM_Def(root : Root) ? (root.process.cycle->filter(SVM)->size() > 0 and root.process.cycle->filter(Cross_Validation)->size() > 0)]
def build_SVM(SVM_params):  
    SVM_model = SVC(kernel=SVM_params['['/]'kernel'], C=SVM_params['['/]'C'], probability=True)
    return SVM_model
[/template]

[template public Random_Forest_Def(root : Root) ? (root.process.cycle->filter(Random_Forest)->size() > 0 and root.process.cycle->filter(Cross_Validation)->size() > 0)]
def build_RandomForest(RF_params):
    RandomForest_model = RandomForestClassifier(n_estimators=RF_params['['/]'n_estimators'],
                                                 max_depth=RF_params.get('max_depth', None),
                                                 random_state=RF_params.get('random_state', None))
    return RandomForest_model
[/template]

[template public Logestic_Regression_Def(root : Root) ? (root.process.cycle->filter(Logistic_Regression)->size() > 0 and root.process.cycle->filter(Cross_Validation)->size() > 0)]
def build_LogisticRegression(logistic_regression_params):
    Lor = LogisticRegression(**logistic_regression_params)
    return Lor
[/template]

[template public Cross_Validation(root : Root)]
[if (root.process.cycle->filter(Cross_Validation)->size() > 0)]
def perform_cross_validation(X, y, classifier, cv_folds, scoring_metrics):
    cv_results = cross_validate(classifier, X, y, cv=cv_folds, scoring=list(scoring_metrics.keys()), return_train_score=False)
    mean_scores = {metric: np.mean(cv_results['['/]f'test_{metric}']) for metric in scoring_metrics}
    std_scores = {metric: np.std(cv_results['['/]f'test_{metric}']) for metric in scoring_metrics}
    for metric_name in scoring_metrics:
        metric_scores = cv_results['['/]f'test_{metric_name}']
        print(f'{metric_name}: {metric_scores}')
        print(f'Mean {metric_name}: {np.mean(metric_scores):.3f}, Standard Deviation: {np.std(metric_scores):.3f}')

[if (root.process.cycle->filter(Cross_Validation).plots->filter(Scatter)->size() > 0)]
        # Plotting scatter plot for individual scores
        plt.figure(figsize=(10, 6))
        plt.scatter(range(len(metric_scores)), metric_scores, c='blue', alpha=0.7, s=100)  
        plt.title(f'Individual {metric_name} Scores with Cross Validation')
        plt.xlabel('Iteration')
        plt.ylabel(metric_name)
        
        # khotut

        plt.axhline(mean_scores['['/]metric_name], color='red', linestyle='--', label='Mean')  
        plt.axhline(mean_scores['['/]metric_name] + std_scores['['/]metric_name], color='green', linestyle=':', label='Mean + Std')  
        plt.axhline(mean_scores['['/]metric_name] - std_scores['['/]metric_name], color='green', linestyle=':', label='Mean - Std')  
        plt.legend()  
        
        plt.show()
[/if]

[if (root.process.cycle->filter(Cross_Validation).plots->filter(Bar)->size() > 0)]
    # Plotting bar plot for mean scores
    plt.figure(figsize=(10, 6))
    sns.barplot(x=list(scoring_metrics.values()), y=list(mean_scores.values()), palette='viridis')
    plt.title('Mean Scores with Cross Validation')
    plt.xlabel('Metrics')
    plt.ylabel('Mean Score')
    plt.show()
[/if]


    return cv_results
[/if]

[/template]

		

[template public Process_Data(root : Root)]
def process_data([for (it : Cycle | process.cycle) separator (', ')][if ((it.oclIsTypeOf(CSV)) or (it.oclIsTypeOf(Excel)))]data=None[/if][if (it.oclAsType(Import_Data).role->filter(Target)->size()>0)],target_column=None[/if][if (it.oclIsTypeOf(Simple_Imputer)and not gettest())]impute_configurations=None[swichtest()/][/if][if (it.oclIsKindOf(Scaling) and not gettest1())]scaling_config=None[swichtest1()/][/if][if (it.oclIsTypeOf(Duplicates))]remove_duplicates_columns=None[/if][if (it.oclIsTypeOf(KNN_Imputer))]impute_configurations_KNN=None[/if][if (it.oclIsTypeOf(OneHot))]one_hot_columns=None[/if][if (it.oclIsTypeOf(Date_to_Int))]extract_date_component=None[/if][if (it.oclIsTypeOf(PCA))]pca_components=None[/if][if ((it.oclIsTypeOf(KNN)) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]KNN_params=None[/if][if (it.oclIsTypeOf(SVM) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]SVM_params=None[/if][if (it.oclIsTypeOf(Random_Forest) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]RF_params=None[/if][if (it.oclIsTypeOf(Logistic_Regression) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]LogisticRegression=None[/if][if (it.oclIsTypeOf(MLP) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]MLP_params=None[/if][if (it.oclIsTypeOf(Voting_Classifier))]voting_params=None[/if][if (it.oclIsTypeOf(Cross_Validation))]cv_folds=None[/if][if (it.oclIsTypeOf(K_Means))]KMeans_params=None[/if][if (it.oclIsTypeOf(K_Medians))]KMedians_params=None[/if][if (it.oclIsTypeOf(DBSCAN))]DBSCAN_params=None[/if][if (it.oclIsTypeOf(OPtics))]OPTICS_params=None[/if][/for][swichtest()/][swichtest1()/]): 
	executed_functions = ['['/]]
[if (root.process.cycle.oclIsKindOf(Classification) <> null)]
	model = None	
[/if]
[if (root.process.cycle->reject(temp : Cycle | temp.oclIsTypeOf(mLmodel::CSV))->size()>0)]

	a =['['/][for (it : Cycle | process.cycle) separator (',')]'[it.after.eClass().name/]'[/for]]
[/if]

	for letter in range (len(a)):
[if (root.process.cycle->filter(Simple_Imputer)->size()>0)]
[for (it : Cycle | root.process.cycle->filter(Simple_Imputer)->first())]
		if a['['/]letter] == 'Simple_Imputer': 
			data = fill_missing_values(data, impute_configurations)
			executed_functions.append('fill_missing_values')
[if (root.process.cycle->filter(Simple_Imputer)->first().print_data = true)]
			print(data.head())
[/if]

[if (root.process.cycle->filter(Simple_Imputer)->first().save_file = true)]
			Simple_Imputer_data = 'Simple_Imputer_data.csv'
			data.to_csv(Simple_Imputer_data, index=False)  
[/if]
[/for]
[/if]


[comment KNN Imputer /]
[if (root.process.cycle->filter(KNN_Imputer)->size()>0)]
		if a['['/]letter] == 'KNN_Imputer':
			knn_config = impute_configurations_KNN['['/]'KNN']
			data = knn_impute_missing_values(
				data=data,
				column_names=knn_config['['/]'column_names'],
				n_neighbors=knn_config.get('n_neighbors', 5),
				weights=knn_config.get('weights', 'uniform'),
				metric=knn_config.get('metric', 'nan_euclidean')
			)
			executed_functions.append('knn_impute_missing_values')
[for (it : Cycle | process.cycle->filter(KNN_Imputer)->first())]
[if (it.oclAsType(KNN_Imputer).save_file=true)]
			data.to_csv('KNN_Imputer.csv', index=False) 
[/if]
[/for]
[/if] 
[comment End KNN Imputer /]

[comment Scaling /]


[if (root.process.cycle->filter(Scaling)->size()>0)]
		if a['['/]letter] == 'scaling_config':
			data = scale_features(data[if (root.process.cycle->filter(CSV).role->filter(Target)->size()>0)], target_column[/if], scaling_config)
			scaling_methods = ['['/]method for method in scaling_config if method != "exclude"]
			executed_functions.extend(scaling_methods)
[if (root.process.cycle->filter(Scaling)->first().print_data = true)]
			print(data.head())
[/if]
[if (root.process.cycle->filter(Scaling)->first().save_file = true)]

			scaled_file_path = 'scaled_data.csv'
			data.to_csv(scaled_file_path, index=False)  
[/if]
[/if] 
[comment End Scaling /]

[comment Date /]

[if (root.process.cycle->filter(Date_to_Int)->size()>0)]

        if a['['/]letter] == 'Date_to_Int': 
            data = extract_date_component_func(data, extract_date_component)
            executed_functions.append('extract_date_component')
[if (root.process.cycle->filter(Date_to_Int)->first().print_data = true)]
			print(data.head())
[/if]
[if (root.process.cycle->filter(Date_to_Int)->first().save_file = true)]
			Date_to_Int_data = 'Date_to_Int.csv'
			data.to_csv(Date_to_Int, index=False)  
[/if]
[/if] 
[comment End Date /]



[comment PCA /]

[if (process.cycle->filter(PCA)->size()>0)]
		if a['['/]letter] == 'PCA':       
			if 'apply_pca' in globals() and callable(apply_pca):
				data = apply_pca(data, pca_components[if (process.cycle->filter(CSV).role->filter(Target)->size() > 0)], target_column[/if])
				executed_functions.append('apply_pca')
[if (root.process.cycle->filter(PCA)->first().print_data = true)]
			print(data.head())
[/if]
[if (root.process.cycle->filter(PCA)->first().save_file = true)]
			PCA_data = 'PCA.csv'
			data.to_csv(PCA_data, index=False)  
[/if]

[/if]
[comment End PCA /]

[comment Duplicates /]
[if (process.cycle->filter(Duplicates)->size()>0)]

		if a['['/]letter] == 'Duplicates':
			data = remove_duplicates(data, remove_duplicates_columns)
			executed_functions.append('remove_duplicates')
			print(data.head())
[if (root.process.cycle->filter(Duplicates)->first().print_data = true)]
			print(data.head())
[/if]
[if (root.process.cycle->filter(Duplicates)->first().save_file = true)]
			Duplicates = 'Duplicates.csv'
			data.to_csv(Duplicates, index=False)  
[/if]
[/if]

[comment End Duplicates /]


[comment OneHot /]
[for (it : Cycle | root.process.cycle)]
[if (it.oclIsTypeOf(OneHot))]
		if a['['/]letter] == 'OneHot': 
			data = one_hot_encoding(data, one_hot_columns)
			executed_functions.append('one_hot_encoding')
[if (root.process.cycle->filter(OneHot)->first().print_data = true)]
			print(data.head())
[/if]
[if (root.process.cycle->filter(OneHot)->first().save_file = true)]
			OneHot = 'OneHot.csv'
			data.to_csv(OneHot, index=False)  
[/if]

[/if]
[/for]
[if (root.process.cycle->filter(Voting_Classifier)->size()=0)]
[KNN_Process_Data(root)/]
[SVM_Process_Data(root)/]
[Random_Forest_Process_Data(root)/]
[MLP_Process_Data(root)/]
[Logestic_Regression_Process_Data(root)/]
[/if]
[Voting_Process_Data(root)/]

[K_Means_Process_Data(root)/]
[DBSCAN_Process_Data(root)/]
[K_Medians_Process_Data(root)/]
[OPTICS_Process_Data(root)/]





	return data, executed_functions, model

[/template]


[comment Send Data /]

[template public Send_Data(root : Root)]
[for (it : Cycle | process.cycle)]
[if (it.oclIsTypeOf(CSV))]
if __name__ == "__main__":
    file_path = '[root.process.file_path/]'
    data = read_csv(file_path)
[if (root.process.cycle->filter(CSV).role->filter(ID)->size() > 0)]
    data = data.drop(columns=['['/][for (it : ID | root.process.cycle->filter(CSV).role->filter(ID)) separator (',')]'[it.name/]'[/for]])
[/if]
[/if]

[if (it.oclIsTypeOf(Excel))]
if __name__ == "__main__":
    file_path = '[root.process.file_path/]'
    data = read_excel(file_path)
[if (root.process.cycle->filter(Excel).role->filter(ID)->size() > 0)]
    data = data.drop(columns=['['/][for (it : ID | root.process.cycle->filter(Excel).role->filter(ID)) separator (',')]'[it.name/]'[/for]])
[/if]
[/if]

[/for]


[if (process.cycle->filter(Simple_Imputer)->size()>0)]
    impute_configurations = ['['/]
[for (it : Cycle | process.cycle)]
[for (bi : Simple_Imputer | it.oclAsType(Simple_Imputer))]
        {
            'column_names': ['['/][for (by : Regular | bi.oclAsType(Simple_Imputer).regular) separator (', ')]'[by.name/]'[/for]],

[if (bi.simple_imputer_arguments->filter(Strategy)->size() > 0)]
       		'strategy': '[for (bu : Strategy | bi.simple_imputer_arguments->filter(Strategy))][if (bu.default_value_sim_str = Strategy_Simple_Imputer::mean )]mean[/if][if (bu.default_value_sim_str = Strategy_Simple_Imputer::median )]median[/if][if (bu.default_value_sim_str = Strategy_Simple_Imputer::most_frequent )]most_frequent[/if][if (bu.default_value_sim_str = Strategy_Simple_Imputer::constant )]constant[/if][/for]',
[/if]

[for (cu : Fill_Value | bi.simple_imputer_arguments->filter(Fill_Value))]
[if (cu->size()>0)]
            'fill_value': [cu.default_value_sim_fill/],
[/if]
[/for]

        },
[/for]
[/for]
    ]
[/if]


[comment Scaling /]

[if (root.process.cycle->filter(Scaling)->size()>0)]
    scaling_config = {
[for (it : Scaling | root.process.cycle->filter(Scaling))]
[if (it.oclIsTypeOf(Normalization))]
		'minmax': ['['/][for (by : Regular | it.regular) separator (', ')]'[by.name/]'[/for]],
[/if]

[if (it.oclIsTypeOf(Standardization))]
		'standardization': ['['/][for (by : Regular | it.regular) separator (', ')]'[by.name/]'[/for]],
[/if]

[if (it.oclIsTypeOf(Robust_Scaling))]
		'robust': ['['/][for (by : Regular | it.regular) separator (', ')]'[by.name/]'[/for]],
[/if]
[/for]
}
[/if]
[comment End Scaling /]

[comment Date to Int /]

[if (root.process.cycle->filter(Trasformation)->size()>0)]
	extract_date_component_config = ['['/]
    {
[for (it : Date_to_Int | process.cycle->filter(Date_to_Int))]
        'date_column_name': [for (by : Regular | it.regular) separator (', ')]'[by.name/]'[/for],
        'date_components': ['['/][for (by : Date_Comp | it.date_comp) separator (', ')][if (by.oclIsTypeOf(Year))]'year'[/if][if (by.oclIsTypeOf(Year))]'month'[/if][if (by.oclIsTypeOf(Year))]'day'[/if][/for]],

   },
[/for]
]
[/if]
[comment End Date to Int /]

[comment KNN Imputer /]

[if (root.process.cycle->filter(KNN_Imputer)->size()>0)]
[for (it : KNN_Imputer | process.cycle->filter(KNN_Imputer))]

    impute_configurations_KNN = {
        'KNN': {
 			'column_names':['['/][for (by : Regular | it.regular) separator (', ')]'[by.name/]'[/for]],

[if (it.knn_argumans->filter(N_Neighbors)->size()=1)]
            'n_neighbors': [for (by : N_Neighbors | it.knn_argumans->filter(N_Neighbors))][it.knn_argumans->filter(N_Neighbors).n_neighbors/][/for],
[/if]          

[if (it.knn_argumans->filter(Weights)->size()=1)]
  
			'weights': '[for (by : Weights | it.knn_argumans->filter(Weights))][it.knn_argumans->filter(Weights).weights/][/for]',
[/if]          


[if (it.knn_argumans->filter(Metric)->size()=1)]
            'metric': '[for (by : Metric | it.knn_argumans->filter(Metric))][it.knn_argumans->filter(Metric)/][/for]'
[/if] 
        }
    }
[/for]
[/if]
[comment End KNN Imputer /]

[comment OneHot /]

[for (it : Cycle | process.cycle)]

[if (it.oclIsTypeOf(OneHot))]
    one_hot_columns=['['/][for (by : Regular | it.oclAsType(OneHot).regular) separator (', ')]'[by.name/]'[/for]]
[/if]
[/for]
[comment End OneHot /]

[comment KNN /]

[if ((process.cycle->filter(KNN)->size()>0) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]
    KNN_params = {
        'k': [process.cycle->filter(KNN).k/]
            }
[/if]
[comment End KNN /]

[comment MLP Scikit_Learn /]

[if ((root.process.cycle->filter(MLP)->size()>0) and (root.process.cycle->filter(MLP)->first().library = Neural_Libraries::scikit) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]
[for (bi : MLP | root.process.cycle->filter(MLP))]
[if (bi.oclIsTypeOf(MLP))]
    MLP_params = {
        'hidden_layer_sizes': ([for (by : Hidden_Layer_Sizes | root.process.cycle->filter(MLP).hidden_layer_sizes) separator (', ')][by.neurons/][/for]), 

[for (it : MLP_Argumans | bi.mlp_argumans)]
[if (it.oclIsTypeOf(Activation_MLP))]
        'activation': '[it.oclAsType(Activation_MLP).activation/]', 
[/if] 
[if (it.oclIsTypeOf(Alpha_MLP))]
        'alpha': [it.oclAsType(Alpha_MLP).alpha/],  
[/if] 

        'solver': 'adam',
        'batch_size': 'auto',
        'learning_rate': 'constant',
[if (it.oclIsTypeOf(Max_Iter_MLP))]
        'max_iter': [it.oclAsType(Max_Iter_MLP).max_iter/]
[/if]
[/for]
    }
[/if]
[/for]
[/if]

[comment END MLP Scikit_Learn /]

[comment MLP Keras /]

[if ((root.process.cycle->filter(MLP)->size()>0) and (root.process.cycle->filter(MLP)->first().library = Neural_Libraries::keras) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]
[for (bi : MLP | root.process.cycle->filter(MLP))]
[if (bi.oclIsTypeOf(MLP))]
    MLP_params = {
        'hidden_layer_sizes': (100, 50, 2), 
        'activation': 'relu',  
        'learning_rate': 0.001,
        'batch_size': 32,
        'epochs': 500,
        'verbose': 0
    }
[/if]
[/for]
[/if]
[comment END MLP Keras /]


[comment Random Forest /]
[for (bi : Random_Forest | root.process.cycle->filter(Random_Forest))]

[if (bi.oclIsTypeOf(Random_Forest) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]

    RF_params = {
[for (it : RF_Argumans | bi.rf_argumans)]
[if (it.oclIsTypeOf(N_Estimators_RF))]
        'n_estimators': [it.oclAsType(N_Estimators_RF).n_estimators/] ,
[/if]

[if (it.oclIsTypeOf(Criterion_RF))]
        'criterion': '[it.oclAsType(Criterion_RF).criterion/]',

[/if]
[if (it.oclIsTypeOf(Random_State_RF))]
        'random_state': [it.oclAsType(Random_State_RF).random_state/],
[/if]

    
[/for]
}
[/if]
[/for]

[comment End Random Forest /]


[if ((root.process.cycle->filter(K_Means)->size()>0))]
    KMeans_params = {
        'n_clusters': [root.process.cycle->filter(K_Means).K/],
        'init': 'k-means++',
        'n_init': 10,
        'max_iter': [root.process.cycle->filter(K_Means).max_iter/],
        'random_state': 0
    }
[/if]


[if ((root.process.cycle->filter(K_Medians)->size()>0))]
    KMedians_params = {
        'n_clusters': [root.process.cycle->filter(K_Medians).K/],
        'n_init': 10,
        'max_iter': [root.process.cycle->filter(K_Medians).max_iter/],
        'random_state': 0
    }
[/if]

[if (root.process.cycle->filter(DBSCAN)->size() > 0)]

    DBSCAN_params = {
        'eps': [root.process.cycle->filter(DBSCAN).eps/],  # radius of neighborhood
        'min_samples': [root.process.cycle->filter(DBSCAN).min_samples/],  # minimum number of points required to form a dense region
        'neighbors': 5,
    }
[/if]


[if (root.process.cycle->filter(OPtics)->size() > 0)]

    OPTICS_params = {
        'eps': [root.process.cycle->filter(OPtics).eps/],  # radius of neighborhood
        'min_samples': [root.process.cycle->filter(OPtics).min_samples/],  # minimum number of points required to form a dense region
        'xi': 0.05  # minimum separation between clusters
    }
[/if]

[comment Voting /]

[if (root.process.cycle->filter(Voting_Classifier)->size()>0)]

    voting_params = {
[if (root.process.cycle->filter(Voting_Classifier).classification->filter(Random_Forest)->size() > 0)]
[for (lm : Random_Forest | process.cycle->filter(Random_Forest))]

[for (rf : RF_Argumans | lm.rf_argumans)]

        'random_forest': {
[if (rf.oclIsTypeOf(N_Estimators_RF))]

            'n_estimators': [rf.oclAsType(N_Estimators_RF).n_estimators/],
[/if]
            'max_depth': 10,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'random_state': [rf.oclAsType(Random_State_RF).random_state/]
[/for]

        },	
[/for]
[/if]
[if (root.process.cycle->filter(Voting_Classifier).classification->filter(SVM)->size() > 0)]
        'svc': {
            'kernel': '[root.process.cycle->filter(Voting_Classifier).classification->filter(SVM).kernel/]',
            'C': [root.process.cycle->filter(Voting_Classifier).classification->filter(SVM).c/],
            'gamma': 'scale',
            'probability': True
        },	
[/if]
[if (root.process.cycle->filter(Voting_Classifier).classification->filter(KNN)->size() > 0)]
        'knn': {
            'n_neighbors': [root.process.cycle->filter(Voting_Classifier).classification->filter(KNN).k/],
            'weights': 'uniform',
            'algorithm': 'auto'
        }	
[/if]
    }
[/if]

[comment END Voting /]

[comment Logestic Regression /]
[if ((root.process.cycle->filter(Logistic_Regression)->size()>0) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]
    logistic_regression_params = {
        'C': 1.0,                  
        'penalty': 'l2',          
        'solver': 'lbfgs',         
        'max_iter': 100,          
        'class_weight': None,     
        'random_state': None,     
        'multi_class': 'auto',      
        'warm_start': False,        
        'tol': 1e-4,                
        'n_jobs': -1,               
        'verbose': 0,               
        'dual': False,              
        'fit_intercept': True,     
    }
[/if]

[comment End Logestic Regression /]


[comment SVM /]
[if ((root.process.cycle->filter(SVM)->size()>0) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]

    SVM_params = {
        'kernel': '[root.process.cycle->filter(SVM)->first().kernel/]',
        'C': [root.process.cycle->filter(SVM)->first().c/] ,
    }
[/if]
[comment End SVM /]

[for (it : Cycle | process.cycle)]
[if (it.oclIsTypeOf(Duplicates))]
    remove_duplicates_columns = {

[if (it->filter(Duplicates).regular->size() > 0)]

        'subset': ['['/][for (by : Regular | it.oclAsType(Duplicates).regular) separator (', ')]'[by.name/]'[/for]],
[/if]
[if (it->filter(Duplicates).duplicates_argumans->filter(Keep)->size() > 0)]
        'keep': [for (bu : Keep | it->filter(Duplicates).duplicates_argumans->filter(Keep))][if (bu.dup_keep = Dup_Arg_Keep::first)]'first'[/if][if (bu.dup_keep = Dup_Arg_Keep::last)]'last'[/if][if (bu.dup_keep = Dup_Arg_Keep::False)]False[/if][/for],
[/if]
    }
[/if]
[/for]

[if (process.cycle->filter(Cross_Validation)->size() > 0)]
    cv_folds = [process.cycle->filter(Cross_Validation).number_of_folds/]
[/if]
[if (process.cycle.oclIsTypeOf(Cross_Validation) <> null)]
    scoring_metrics = {
[if (process.cycle->filter(Cross_Validation)->first().accuracy=true)]
	    'accuracy': 'Accuracy',
[/if]
[if (process.cycle->filter(Cross_Validation)->first().precision=true)]
        'precision_macro': 'Precision Macro',
[/if]
[if (process.cycle->filter(Cross_Validation)->first().recall=true)]
        'recall_macro': 'Recall Macro',
[/if]
[if (process.cycle->filter(Cross_Validation)->first().f1_score=true)]
        'f1_macro': 'F1 Macro'
[/if]
    }
[/if]

    processed_data, executed_functions[if ((root.process.cycle->filter(Classification)->size() > 0)) or (root.process.cycle->filter(Clustering)->size() > 0)], model[/if] = process_data(

[let var : Set(Cycle) = process.cycle->asSequence()->asSet()]

[for (it : Cycle | var->asSequence()->sortedBy(it | it.toString())->asOrderedSet())]
    [if (it.oclIsTypeOf(CSV))]
        data=data,
    [/if]

    [if (it.oclIsTypeOf(OneHot))]
        one_hot_columns=one_hot_columns,
    [/if]
 
    [if (it.oclAsType(CSV).role->filter(Target)->size()>0)]
        target_column='[it.oclAsType(CSV).role->filter(Target).name/]',
    [/if]

    [if (it->filter(Simple_Imputer)->size()>0 and not gettest())]
   	 impute_configurations=impute_configurations,
		[swichtest()/]
	 [/if]
    [if (it.oclIsTypeOf(KNN_Imputer))]
        impute_configurations_KNN=impute_configurations_KNN,
    [/if]
    [if ((it.oclIsTypeOf(KNN)) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]

        KNN_params=KNN_params,
	[/if]

[if ((it.oclIsTypeOf(SVM)) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]
        SVM_params=SVM_params,
[/if]

[if ((it.oclIsTypeOf(MLP)) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]
        MLP_params=MLP_params,
[/if]

[if ((it.oclIsTypeOf(Random_Forest)) and (root.process.cycle->filter(Voting_Classifier)->size()=0))]

        RF_params=RF_params,
[/if]

[if (it.oclIsTypeOf(Voting_Classifier))]

        voting_params=voting_params,
[/if]

[if (it.oclIsTypeOf(Cross_Validation))]
		cv_folds=cv_folds,
[/if]

[if (it->filter(Scaling)->size()>0 and not gettest1())]
        scaling_config=scaling_config,
		[swichtest1()/]

[/if]

[if (it->filter(Duplicates)->size()>0)]
        remove_duplicates_columns=remove_duplicates_columns,
[/if]

[if (it.oclIsTypeOf(Date_to_Int))]
        extract_date_component=extract_date_component_config,
[/if]

	[if (it.oclIsTypeOf(PCA))]
	[let bc : PCA = it.oclAsType(PCA)]
        pca_components=[if (bc.pca_argumans.oclIsTypeOf(Component)->size() > 0)][bc.pca_argumans.oclAsType(Component).component_value/][else][bc.pca_argumans.oclAsType(Varience).varience_value/][/if],

	[/let]
	[/if]

[if (it.oclIsTypeOf(K_Means))]
        KMeans_params=KMeans_params,

[/if]

[if (it.oclIsTypeOf(OPtics))]
        OPTICS_params=OPTICS_params,
[/if]

[if (it.oclIsTypeOf(DBSCAN))]
        DBSCAN_params=DBSCAN_params,
[/if]

[if (it.oclIsTypeOf(K_Medians))]
        KMedians_params=KMedians_params,
[/if]

[/for]
		[swichtest()/]
		[swichtest1()/]

[/let]

    )
[/template]


[comment Proceess Data algorithms /]
[template public KNN_Process_Data(root : Root)]
[for (var : KNN | process.cycle->filter(KNN)->first())]
[if (var.oclIsTypeOf(KNN))]


		if a['['/]letter] == 'KNN':
			y = data['['/]target_column]
			X = data.drop(columns=['['/]target_column])	
     

			KNN_model = build_KNN(KNN_params)
			executed_functions.append('train_KNN')

[if (var.after.oclIsTypeOf(Cross_Validation))]
		if a['['/]letter] == 'Cross_Validation':       

			cv_results = perform_cross_validation(X, y, KNN_model, cv_folds, scoring_metrics)                      
			executed_functions.append('perform_cross_validation')
[/if]
[/if]
[/for]
[/template]




[template public SVM_Process_Data(root : Root)]
[comment SVM /]
[for (var : SVM | root.process.cycle->filter(SVM)->first())]

[if (var.oclIsTypeOf(SVM))]

		if a['['/]letter] == 'SVM': 
			y = data['['/]target_column]
			X = data.drop(columns=['['/]target_column])	      

			SVM_model = build_SVM(SVM_params)
[/if]
			executed_functions.append('train_SVM')

[if (var.after.oclIsTypeOf(Cross_Validation))]
		if a['['/]letter] == 'Cross_Validation':       

			cv_results = perform_cross_validation(X, y, SVM_model, cv_folds, scoring_metrics)                      
			executed_functions.append('perform_cross_validation')

[/if]
[/for]
[comment End SVM /]
[/template]

[template public Voting_Process_Data(root : Root)]

[comment Voting_Classifier /]
[for (var : Voting_Classifier | root.process.cycle->filter(Voting_Classifier)->first())]
[if (var.oclIsTypeOf(Voting_Classifier))]
		if a['['/]letter] == 'Voting_Classifier':
			y = data['['/]target_column]
			X = data.drop(columns=['['/]target_column])       
			voting_model = build_voting_classifier(voting_params)
			executed_functions.append('Voting_Train')
[/if]
[if (var.after.oclIsTypeOf(Cross_Validation))]
		if a['['/]letter] == 'Cross_Validation':       

			cv_results = perform_cross_validation(X, y, voting_model, cv_folds, scoring_metrics)                      
			executed_functions.append('perform_cross_validation')
[/if]
[/for]
[comment End Voting_Classifier /]
[/template]


[template public MLP_Process_Data(root : Root)]
[comment MLP /]
[for (var : MLP | root.process.cycle->filter(MLP)->first())]

[if (var.oclIsTypeOf(MLP))]



		if a['['/]letter] == 'MLP':   
			y = data['['/]target_column]
			X = data.drop(columns=['['/]target_column])	
    
			MLP_model = build_MLP(MLP_params)
			executed_functions.append('MLP_Train')

[if (var.after.oclIsTypeOf(Cross_Validation))]
		if a['['/]letter] == 'Cross_Validation':       
			cv_results = perform_cross_validation(X, y, MLP_model, cv_folds, scoring_metrics)                      
			executed_functions.append('perform_cross_validation')

[/if]
[/if]
[/for]
[comment End MLP /]
[/template]


[template public Logestic_Regression_Process_Data(root : Root)]
[comment MLP /]
[for (var : Logistic_Regression | root.process.cycle->filter(Logistic_Regression)->first())]

[if (var.oclIsTypeOf(Logistic_Regression))]



		if a['['/]letter] == 'LogisticRegression':
			y = data['['/]target_column]
			X = data.drop(columns=['['/]target_column])   
			LRegression = build_LogisticRegression(logistic_regression_params)
			executed_functions.append('build_LogisticRegression')

[if (var.after.oclIsTypeOf(Cross_Validation))]
		if a['['/]letter] == 'Cross_Validation':       
			cv_results = perform_cross_validation(X, y, LRegression, cv_folds, scoring_metrics)                      
			executed_functions.append('perform_cross_validation')

[/if]
[/if]
[/for]
[comment End MLP /]
[/template]

[template public Random_Forest_Process_Data(root : Root)]

[comment Random Forest /]
[for (var : Random_Forest | root.process.cycle->filter(Random_Forest)->first())]

[if (var.oclIsTypeOf(Random_Forest))]
	

		if a['['/]letter] == 'Random_Forest': 
			y = data['['/]target_column]
			X = data.drop(columns=['['/]target_column])      

			RandomForest_model = build_RandomForest(RF_params)

			executed_functions.append('train_RandomForest')

[if (var.after.oclIsTypeOf(Cross_Validation))]
		if a['['/]letter] == 'Cross_Validation':       

			cv_results = perform_cross_validation(X, y, RandomForest_model, cv_folds, scoring_metrics)                      
			executed_functions.append('perform_cross_validation')
[/if]
[/if]
[/for]
[comment End Random Forest /]
[/template]

[template public K_Means_Process_Data(root : Root) ? ((root.process.cycle->filter(K_Means)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(K_Means)->first().library=Library_Clustering::pyclustering))]
[comment K_Means PyClustering /]

[for (var : Clustering | root.process.cycle->filter(Clustering))]

[if (var.oclIsTypeOf(K_Means))]


		if a['['/]letter] == 'K_Means':
			labels, cluster_centers = build_kmeans(KMeans_params, data)
			model = {'labels': labels, 'cluster_centers': cluster_centers}
			executed_functions.append('build_kmeans')

[/if]
[/for]
[/template]

[template public K_Means_Process_Data(root : Root) ? ((root.process.cycle->filter(K_Means)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(K_Means)->first().library=Library_Clustering::scikit))]
[for (var : Clustering | root.process.cycle->filter(Clustering))]
[if (var.oclIsTypeOf(K_Means))]
		if a['['/]letter] == 'K_Means':
			kmeans_model, centroids_mean = build_kmeans(KMeans_params, data)
			model = kmeans_model
			executed_functions.append('build_kmeans')

	if model is not None:
		print("Mean of centroids of each cluster:")
		for i, mean in enumerate(centroids_mean):
			print(f"Cluster {i+1}: {mean}")



[/if]
[/for]
[/template]


[template public K_Medians_Process_Data(root : Root) ? ((root.process.cycle->filter(K_Medians)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(K_Medians)->first().library=Library_Clustering::pyclustering))]

[comment K_Medians PyClustering /]
[for (var : Clustering | process.cycle->filter(Clustering))]

[if (var.oclIsTypeOf(K_Medians))]
		if a['['/]letter] == 'K_Medians':      
			labels, cluster_medians = build_kmedians(KMedians_params, data)
			model = {'labels': labels, 'cluster_medians': cluster_medians}
			executed_functions.append('build_kmedians')
[/if]
[/for]
[/template]

[template public OPTICS_Process_Data(root : Root) ? ((root.process.cycle->filter(OPtics)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(OPtics)->first().library=Library_Clustering::pyclustering))]

[for (var : Clustering | process.cycle->filter(Clustering))]

[if (var.oclIsTypeOf(OPtics))]
		if a['['/]letter] == 'OPtics':
			labels = build_optics(OPTICS_params, data)
			model = {'labels': labels}
			executed_functions.append('build_optics')
[/if]
[/for]
[/template]


[comment DBSCAN PyClustering /]
[template public DBSCAN_Process_Data(root : Root) ? ((root.process.cycle->filter(DBSCAN)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(DBSCAN)->first().library=Library_Clustering::pyclustering))]
[for (var : Clustering | root.process.cycle->filter(Clustering))]

[if (var.oclIsTypeOf(DBSCAN))]
		if a['['/]letter] == 'DBSCAN':
			labels = build_dbscan(DBSCAN_params, data)
			model = {'labels': labels}
			executed_functions.append('build_dbscan')
[/if]
[/for]
[/template]


[template public MLP_Def(root : Root) ? (root.process.cycle->filter(MLP)->size() > 0 and root.process.cycle->filter(Cross_Validation)->size() > 0 and root.process.cycle->filter(MLP)->first().library = Neural_Libraries::scikit )]
def build_MLP(MLP_params):  
    MLP_model = MLPClassifier(hidden_layer_sizes=MLP_params['['/]'hidden_layer_sizes'],
                              activation=MLP_params['['/]'activation'],
                              solver=MLP_params['['/]'solver'],
                              alpha=MLP_params['['/]'alpha'],
                              batch_size=MLP_params['['/]'batch_size'],
                              learning_rate=MLP_params['['/]'learning_rate'],
                              max_iter=MLP_params['['/]'max_iter'])
    return MLP_model
[/template]

[template public MLP_Def(root : Root) ? (root.process.cycle->filter(MLP)->size() > 0 and root.process.cycle->filter(Cross_Validation)->size() > 0 and root.process.cycle->filter(MLP)->first().library = Neural_Libraries::keras )]
import tensorflow as tf
def build_MLP(hidden_layer_sizes=(100, 50, 2), activation='relu', learning_rate=0.001):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(units=hidden_layer_sizes['['/]0], activation=activation, input_dim=2))
    for units in hidden_layer_sizes['['/]1:]:
        model.add(tf.keras.layers.Dense(units=units, activation=activation))
    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['['/]'accuracy'])
    return model
[/template]

[template public K_means(root : Root) ? ((root.process.cycle->filter(K_Means)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(K_Means)->first().library=Library_Clustering::scikit))]
[if (root.process.cycle->filter(K_Means)->size()>0)]
from sklearn.cluster import KMeans
def build_kmeans(KMeans_params, data):
    kmeans_model = KMeans(
        n_clusters=KMeans_params['['/]'n_clusters'],
        init=KMeans_params.get('init', 'k-means++'),
        n_init=KMeans_params.get('n_init', 10),
        max_iter=KMeans_params.get('max_iter', 300),
        random_state=KMeans_params.get('random_state', None)
    )
    kmeans_model.fit(data)
    
    # Get cluster centroids
    centroids = kmeans_model.cluster_centers_
    
    # Calculate the mean of centroids
    centroids_mean = centroids.mean(axis=1)
    
    return kmeans_model, centroids_mean

[/if]
[/template]

[template public K_means(root : Root) ? ((root.process.cycle->filter(K_Means)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(K_Means)->first().library=Library_Clustering::pyclustering))]
def build_kmeans(KMeans_params, data):
    initial_centers = kmeans_plusplus_initializer(data.values, KMeans_params['['/]'n_clusters']).initialize()
    kmeans_instance = kmeans(data.values, initial_centers)
    kmeans_instance.process()
    clusters = kmeans_instance.get_clusters()
    cluster_centers = kmeans_instance.get_centers()
    labels = ['['/]-1] * len(data)
    for cluster_idx, cluster in enumerate(clusters):
        for sample_idx in cluster:
            labels['['/]sample_idx] = cluster_idx
    
    # Calculate statistics for cluster centers
    cluster_centers_stats = {
        f'Cluster {idx + 1} Mean': np.mean(center) for idx, center in enumerate(cluster_centers)
    }
    
    # Print cluster centers statistics
    print("Cluster Centers Statistics:")
    for key, value in cluster_centers_stats.items():
        print(f"{key}: {value}")
[if (process.cycle->filter(Cluster_Evaluation).scatter->size() > 0)]
  	
    # Plot scatter plot of data points colored by clusters
    plt.figure(figsize=(8, 6))
    for cluster_idx, cluster in enumerate(clusters):
        cluster_data = data.iloc['['/]cluster]
        plt.scatter(cluster_data.iloc['['/]:, 0], cluster_data.iloc['['/]:, 1], label=f'Cluster {cluster_idx + 1}')
    plt.scatter(np.array(cluster_centers)['['/]:, 0], np.array(cluster_centers)['['/]:, 1], color='black', marker='x', label='Cluster Centers')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Scatter Plot of Data Points Colored by Clusters')
    plt.legend()
    plt.show()
[/if]  

    return labels, cluster_centers
[/template]


[template public DBSCAN_pyclustering_Def(root : Root) ? ((root.process.cycle->filter(DBSCAN)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(DBSCAN)->first().library=Library_Clustering::pyclustering))]

def build_dbscan(DBSCAN_params, data):
    dbscan_instance = dbscan(data.values, eps=DBSCAN_params['['/]'eps'], minpts=DBSCAN_params['['/]'min_samples'], neighbors=DBSCAN_params['['/]'neighbors'])
    dbscan_instance.process()
    clusters = dbscan_instance.get_clusters()
    noise = dbscan_instance.get_noise()
    labels = ['['/]-1] * len(data)
    for cluster_idx, cluster in enumerate(clusters):
        for sample_idx in cluster:
            labels['['/]sample_idx] = cluster_idx
    for sample_idx in noise:
        labels['['/]sample_idx] = -1
    
    # Calculate statistics for cluster centers
    cluster_centers = ['['/]]
    for cluster in clusters:
        cluster_center = np.mean(data.iloc['['/]cluster], axis=0)
        cluster_centers.append(cluster_center)

    # Print cluster centers statistics
    print("Cluster Centers Statistics:")
    for idx, center in enumerate(cluster_centers):
        print(f"Cluster {idx + 1} Mean: {center}")


[if (process.cycle->filter(Cluster_Evaluation).scatter->size() > 0)]
  	
    # Plot scatter plot of data points colored by clusters
    plt.figure(figsize=(8, 6))
    for cluster_idx, cluster in enumerate(clusters):
        cluster_data = data.iloc['['/]cluster]
        plt.scatter(cluster_data.iloc['['/]:, 0], cluster_data.iloc['['/]:, 1], label=f'Cluster {cluster_idx + 1}')
    plt.scatter(np.array(cluster_centers)['['/]:, 0], np.array(cluster_centers)['['/]:, 1], color='black', marker='x', label='Cluster Centers')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Scatter Plot of Data Points Colored by Clusters')
    plt.legend()
    plt.show()
[/if]  

    
    return labels, cluster_centers

[/template]

[template public K_Medians_pyclustering_Def(root : Root) ? ((root.process.cycle->filter(K_Medians)->size() > 0) and (root.process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(K_Medians)->first().library=Library_Clustering::pyclustering))]

def build_kmedians(KMedians_params, data):
    initial_medians = random_center_initializer(data.values, KMedians_params['['/]'n_clusters']).initialize()
    kmedians_instance = kmedians(data.values, initial_medians)
    kmedians_instance.process()
    clusters = kmedians_instance.get_clusters()
    medians = kmedians_instance.get_medians()
    labels = kmedians_instance.predict(data.values) 

    # Calculate cluster statistics
    cluster_stats = ['['/]]
    for cluster_idx, cluster in enumerate(clusters):
        cluster_data = data.iloc['['/]cluster]
        cluster_mean = cluster_data.mean()
        cluster_std = cluster_data.std()
        cluster_stats.append({'Cluster': cluster_idx + 1, 'Mean': cluster_mean, 'Std': cluster_std})
    
    # Print cluster statistics
    print("Cluster Statistics:")
    for stat in cluster_stats:
        print(f"Cluster {stat['['/]'Cluster']} Mean: {stat['['/]'Mean']}")
        print(f"Cluster {stat['['/]'Cluster']} Standard Deviation: {stat['['/]'Std']}")
    
[if (process.cycle->filter(Cluster_Evaluation).scatter->size() > 0)]

    # Plot scatter plot of data points colored by clusters
    plt.figure(figsize=(8, 6))
    for cluster_idx, cluster in enumerate(clusters):
        cluster_data = data.iloc['['/]cluster]
        plt.scatter(cluster_data.iloc['['/]:, 0], cluster_data.iloc['['/]:, 1], label=f'Cluster {cluster_idx + 1}')
    plt.scatter(np.array(medians)['['/]:, 0], np.array(medians)['['/]:, 1], color='black', marker='x', label='Cluster Medians')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Scatter Plot of Data Points Colored by Clusters')
    plt.legend()
    plt.show()
[/if]    
    return labels, medians

[/template]


[template public Optics_pyclustering_Def(root : Root) ? ((process.cycle->filter(OPtics)->size() > 0) and (process.cycle->filter(Cross_Validation)->size() = 0) and (root.process.cycle->filter(OPtics)->first().library=Library_Clustering::pyclustering))]

def build_optics(OPTICS_params, data):
    optics_instance = optics(data.values, eps=OPTICS_params['['/]'eps'], minpts=OPTICS_params['['/]'min_samples'], xi=OPTICS_params['['/]'xi'])
    optics_instance.process()
    clusters = optics_instance.get_clusters()
    noise = optics_instance.get_noise()
    labels = ['['/]-1] * len(data)
    for cluster_idx, cluster in enumerate(clusters):
        for sample_idx in cluster:
            labels['['/]sample_idx] = cluster_idx
    for sample_idx in noise:
        labels['['/]sample_idx] = -1
    
    # Calculate statistics for cluster centers
    cluster_centers = ['['/]]
    for cluster in clusters:
        cluster_center = np.mean(data.iloc['['/]cluster], axis=0)
        cluster_centers.append(cluster_center)

    # Print cluster centers statistics
    print("Cluster Centers Statistics:")
    for idx, center in enumerate(cluster_centers):
        print(f"Cluster {idx + 1} Mean: {center}")

[if (process.cycle->filter(Cluster_Evaluation).scatter->size() > 0)]

    # Plot scatter plot of data points colored by clusters
    plt.figure(figsize=(8, 6))
    for cluster_idx, cluster in enumerate(clusters):
        cluster_data = data.iloc['['/]cluster]
        plt.scatter(cluster_data.iloc['['/]:, 0], cluster_data.iloc['['/]:, 1], label=f'Cluster {cluster_idx + 1}')
    plt.scatter(np.array(cluster_centers)['['/]:, 0], np.array(cluster_centers)['['/]:, 1], color='black', marker='x', label='Cluster Centers')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Scatter Plot of Data Points Colored by Clusters')
    plt.legend()
    plt.show()
[/if]
[/template]



[template public Voting_Def(root : Root) ? (process.cycle->filter(Voting_Classifier)->size() > 0 and process.cycle->filter(Cross_Validation)->size() > 0)]

[if (process.cycle->filter(Voting_Classifier).classification->filter(Random_Forest)->size() > 0)]
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
[/if]
[if (process.cycle->filter(Voting_Classifier).classification->filter(SVM)->size() > 0)]
from sklearn.svm import SVC
[/if]
[if (process.cycle->filter(Voting_Classifier).classification->filter(KNN)->size() > 0)]
from sklearn.neighbors import KNeighborsClassifier
[/if]

def build_voting_classifier(voting_params):
    estimators = ['['/]]
[if (process.cycle->filter(Voting_Classifier).classification->filter(Random_Forest)->size() > 0)]

    # RandomForestClassifier
    rf_params = voting_params.get('random_forest', {})
    rf_estimator = ('rf', RandomForestClassifier(**rf_params))
    estimators.append(rf_estimator)
[/if]

[if (process.cycle->filter(Voting_Classifier).classification->filter(SVM)->size() > 0)]
    # SVC
    svc_params = voting_params.get('svc', {})
    svc_estimator = ('svc', SVC(**svc_params))
    estimators.append(svc_estimator)
[/if]

[if (process.cycle->filter(Voting_Classifier).classification->filter(KNN)->size() > 0)]
    # KNeighborsClassifier
    knn_params = voting_params.get('knn', {})
    knn_estimator = ('knn', KNeighborsClassifier(**knn_params))
    estimators.append(knn_estimator)
[/if]

[if (process.cycle->filter(Voting_Classifier)->first().voting = voting_pred::soft)]
    voting = VotingClassifier(estimators, voting='soft')
[/if]
[if (process.cycle->filter(Voting_Classifier)->first().voting = voting_pred::hard)]
    voting = VotingClassifier(estimators, voting='hard')
[/if]
    return voting
[/template]


[template public Auto_Prep(root : Root)]
[if (process.cycle->filter(Auto_Prep)->size() > 0)]
data = pd.read_csv('dataset.csv')

[if (root.process.cycle->filter(Auto_Prep)->first().duplicates = true)]
data.drop_duplicates(inplace=True)
[/if]

[if (root.process.cycle->filter(Auto_Prep)->first().drop = true)]
data.drop(columns=['['/][root.process.cycle->filter(Auto_Prep).miss_id->sep(',')/]], inplace=True)
[/if]



float = ['['/][for (it : Regular | root.process.cycle->filter(Auto_Prep).regular) separator (',')][if (it.type = Attribiutes_Type::float)]'[it.name/]'[/if][/for]]
categorical_features = ['['/]'categorical_feature1', 'categorical_feature2']
text_features = ['['/]'text_feature']



[/if]





[/template]





