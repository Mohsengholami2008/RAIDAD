

import json
import time
import csv
from pathlib import Path

from typing import Dict
from typing import Tuple
from typing import Optional

import numpy as np

import paho.mqtt.client as mqtt

from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier, Perceptron

from sklearn.linear_model import SGDClassifier



import logging




MQTT_BROKER = "localhost"
MQTT_PORT = "1883"



MQTT_SENSOR_TOPIC = "control/mode"   
MQTT_CONTROL_TOPIC = "sensors/+/value"  



REQUIRED_DEVICES = ["dev", "dev1"]

SLOT_SIZE_SECONDS = 5

TRAIN_ROW_LIMIT = 100

CLASSES = np.array([0, 1])

METRICS_CSV_PATH = Path("metrics_log.csv")

METRICS_LOG_EVERY = 1  

SENSOR_VALUE_MIN = -1e3
SENSOR_VALUE_MAX = 1e6

ROBUST_BUFFER_MAX_ROWS = 500



MODEL_CONFIG = {

	"preprocess_steps": ["normalize"],
    "model_type": "SGDClassifier",
}


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)



class RowBuffer:


    def __init__(self, slot_id: int):
        self.slot_id = slot_id
        self.messages: Dict[str, dict] = {}  # device_id -> data dict

    def add_message(self, device_id: str, data: dict) -> None:
        self.messages[device_id] = data

    def is_complete(self) -> bool:
        return all(d in self.messages for d in REQUIRED_DEVICES)

    def build_row(self) -> Tuple[np.ndarray, Optional[int]]:

        values = []
        label = None

        for dev in REQUIRED_DEVICES:
            msg = self.messages.get(dev)
            if msg is None:
                values.append(np.nan)
                continue

            val = msg.get("value")
            if val is None:
                values.append(np.nan)
            else:
                values.append(float(val))

            if label is None and "label" in msg and msg["label"] is not None:
                try:
                    label = int(msg["label"])
                except (TypeError, ValueError):
                    pass

        X_row = np.array(values, dtype=float)
        return X_row, label


# =============================
# کلاس Preprocessor: پیش‌پردازش به صورت pipeline
# =============================

class Preprocessor:
    """
    پیش‌پردازش به صورت pipeline:
      مراحل مجاز در لیست steps:
        - "clean"  : پاکسازی پایه (NaN، clip، ... )
        - "diffs"  : اضافه کردن اختلاف بین تمام جفت ویژگی‌ها
        - "poly2"  : PolynomialFeatures درجه ۲ روی کل بردار
        - "minmax" : نرمال‌سازی Min-Max به‌صورت آنلاین روی بردار فعلی

      مراحل اسکیلینگ (standard / normalize / robust_scaling)
      در این کلاس نادیده گرفته می‌شوند و در OnlineModelManager اعمال می‌شوند.
    """

    def __init__(self, steps: Union[str, List[str]]):
        # steps می‌تواند رشته یا لیست باشد → اینجا همیشه لیستش می‌کنیم
        if isinstance(steps, str):
            steps = [steps]
        self.steps: List[str] = [s.lower() for s in steps]

        self.poly: Optional[PolynomialFeatures] = None
        self.poly_initialized = False

        # برای minmax
        self.minmax_initialized = False
        self.min_: Optional[np.ndarray] = None
        self.max_: Optional[np.ndarray] = None

    def _clean_basic(self, X_row: np.ndarray) -> np.ndarray:
        X = X_row.astype(float).copy()

        if np.all(np.isnan(X)):
            X[:] = 0.0
            return X

        # جایگزینی NaN با میانگین
        nan_mask = np.isnan(X)
        if np.any(nan_mask):
            mean_val = np.nanmean(X)
            X[nan_mask] = mean_val

        # clip
        X = np.clip(X, SENSOR_VALUE_MIN, SENSOR_VALUE_MAX)
        return X

    def _add_diffs(self, X: np.ndarray) -> np.ndarray:
        # اختلاف بین تمام زوج‌های ویژگی
        n = X.shape[0]
        if n < 2:
            return X

        diffs = []
        for i in range(n):
            for j in range(i + 1, n):
                diffs.append(X[i] - X[j])

        diffs = np.array(diffs, dtype=float)
        return np.concatenate([X, diffs])

    def _apply_poly2(self, X: np.ndarray) -> np.ndarray:
        # PolynomialFeatures روی کل بردار X
        if X.ndim == 1:
            X_2d = X.reshape(1, -1)
        else:
            X_2d = X

        if not self.poly_initialized:
            self.poly = PolynomialFeatures(degree=2, include_bias=False)
            self.poly.fit(X_2d)
            self.poly_initialized = True

        X_poly = self.poly.transform(X_2d)[0]
        return X_poly

    def _minmax_scale(self, X: np.ndarray) -> np.ndarray:
        X = X.astype(float)

        if not self.minmax_initialized:
            self.min_ = X.copy()
            self.max_ = X.copy()
            self.minmax_initialized = True
        else:
            self.min_ = np.minimum(self.min_, X)
            self.max_ = np.maximum(self.max_, X)

        denom = self.max_ - self.min_
        denom[denom == 0] = 1.0  # جلوگیری از تقسیم بر صفر

        X_scaled = (X - self.min_) / denom
        return X_scaled

    def transform(self, X_row: np.ndarray) -> np.ndarray:
        X = X_row.astype(float).copy()

        # نام‌های مراحل اسکیلینگ که باید در این کلاس نادیده گرفته شوند
        scaling_step_names = {
            "scale", "standard", "standardization", "standard_scaler",
            "normalize", "normalization", "l2norm", "l2_norm",
            "robust", "robust_scaling", "robust_scale", "robust_scaler",
        }

        # به ترتیب steps اجرا می‌کنیم
        for step in self.steps:
            s = step.lower()
            if s == "clean":
                X = self._clean_basic(X)
            elif s == "diffs":
                X = self._add_diffs(X)
            elif s == "poly2":
                X = self._apply_poly2(X)
            elif s == "minmax":
                X = self._minmax_scale(X)
            elif s in scaling_step_names:
                # این مراحل در OnlineModelManager هندل می‌شوند
                logging.debug(
                    f"Step '{step}' در Preprocessor نادیده گرفته شد "
                    f"(اسکیلینگ در OnlineModelManager اعمال می‌شود)."
                )
            else:
                logging.warning(f"مرحله پیش‌پردازش ناشناخته: {step}")

        return X

# =============================
# ساخت مدل بر اساس MODEL_TYPE
# =============================

def build_classifier(model_type: str):
    mt = model_type.lower()
    if mt == "logistic_sgd":
        return SGDClassifier(
            loss="log_loss",
            learning_rate="optimal",
            random_state=0
        )
    elif mt == "linear_svm":
        return SGDClassifier(
            loss="hinge",
            learning_rate="optimal",
            random_state=0
        )
    elif mt == "pa":
        return PassiveAggressiveClassifier(random_state=0)
    elif mt == "perceptron":
        return Perceptron(random_state=0)
    else:
        logging.warning(f"MODEL_TYPE ناشناخته: {model_type}، از logistic_sgd استفاده می‌کنیم.")
        return SGDClassifier(
            loss="log_loss",
            learning_rate="optimal",
            random_state=0
        )

# =============================
# کلاس OnlineModelManager
# =============================

class OnlineModelManager:
    """
    مدیریت مدل آنلاین (Preprocessor + [اختیاری] Scaling + Classifier) به‌صورت partial_fit،
    همراه با نگهداری وضعیت TRAIN/TEST و لاگ متریک‌ها.

    نوع اسکیلینگ با preprocess_steps تنظیم می‌شود:
      - Standardization  → "standard", "scale", "standardization", "standard_scaler"
      - Normalization    → "normalize", "normalization", "l2norm", "l2_norm"
      - Robust_Scaling   → "robust", "robust_scaling", "robust_scale", "robust_scaler"
    """

    def __init__(self, model_config: dict):
        preprocess_steps_cfg = model_config.get("preprocess_steps", ["clean"])
        model_type = model_config.get("model_type", "logistic_sgd")

        # نرمال‌سازی ورودی steps به لیست
        if isinstance(preprocess_steps_cfg, str):
            preprocess_steps_cfg = [preprocess_steps_cfg]

        steps_lower = [s.lower() for s in preprocess_steps_cfg]

        # تشخیص نوع اسکیلینگ
        standard_names = {"scale", "standard", "standardization", "standard_scaler"}
        normalize_names = {"normalize", "normalization", "l2norm", "l2_norm"}
        robust_names = {"robust", "robust_scaling", "robust_scale", "robust_scaler"}

        self.scaler_type: Optional[str] = None  # "standard" | "normalize" | "robust" | None

        for s in steps_lower:
            if s in standard_names:
                if self.scaler_type is None:
                    self.scaler_type = "standard"
                elif self.scaler_type != "standard":
                    logging.warning(
                        f"Multiple scaling types درخواست شده‌اند (قبلی={self.scaler_type}, جدید=standard). "
                        f"از {self.scaler_type} استفاده می‌کنیم."
                    )
            elif s in normalize_names:
                if self.scaler_type is None:
                    self.scaler_type = "normalize"
                elif self.scaler_type != "normalize":
                    logging.warning(
                        f"Multiple scaling types درخواست شده‌اند (قبلی={self.scaler_type}, جدید=normalize). "
                        f"از {self.scaler_type} استفاده می‌کنیم."
                    )
            elif s in robust_names:
                if self.scaler_type is None:
                    self.scaler_type = "robust"
                elif self.scaler_type != "robust":
                    logging.warning(
                        f"Multiple scaling types درخواست شده‌اند (قبلی={self.scaler_type}, جدید=robust). "
                        f"از {self.scaler_type} استفاده می‌کنیم."
                    )

        # لیست مراحل اسکیلینگ (برای حذف از Preprocessor)
        scaling_step_names = standard_names | normalize_names | robust_names

        # steps مربوط به Preprocessor (بدون مراحل اسکیلینگ)
        steps_for_preproc = [s for s in steps_lower if s not in scaling_step_names]

        self.preproc = Preprocessor(steps_for_preproc)
        self.scaler: Optional[Union[StandardScaler, RobustScaler]] = None
        self.scaler_initialized: bool = False

        # بافر برای robust scaling
        self.robust_buffer: List[np.ndarray] = []

        self.clf = build_classifier(model_type)
        self.model_initialized = False

        self.mode = "TRAIN"  # "TRAIN" یا "TEST"
        self.train_rows_seen = 0

        self.test_total = 0
        self.test_correct = 0

        self._init_metrics_csv()

        logging.info(
            f"Initialized OnlineModelManager with MODEL_TYPE={model_type}, "
            f"PREPROCESS_STEPS={steps_for_preproc}, scaler_type={self.scaler_type}"
        )

    # ---------- ابزارهای اسکیلینگ ----------

    @staticmethod
    def _l2_normalize(X_2d: np.ndarray) -> np.ndarray:
        norms = np.linalg.norm(X_2d, axis=1, keepdims=True)
        norms[norms == 0.0] = 1.0
        return X_2d / norms

    def _scale_for_train(self, X_2d: np.ndarray) -> np.ndarray:
        """
        اسکیلینگ در فاز TRAIN.
        - Standard: StandardScaler + partial_fit
        - Normalize: L2 روی هر ردیف (بدون state)
        - Robust: تقریبی، با fit دوره‌ای روی بافر
        """
        if self.scaler_type == "standard":
            if self.scaler is None:
                self.scaler = StandardScaler()
            # آنلاین
            self.scaler.partial_fit(X_2d)
            self.scaler_initialized = True
            return self.scaler.transform(X_2d)

        elif self.scaler_type == "normalize":
            return self._l2_normalize(X_2d)

        elif self.scaler_type == "robust":
            # بافر برای تخمین آماره‌های robust
            self.robust_buffer.append(X_2d[0].copy())
            if len(self.robust_buffer) >= ROBUST_BUFFER_MAX_ROWS:
                data = np.vstack(self.robust_buffer)
                if self.scaler is None:
                    self.scaler = RobustScaler()
                self.scaler.fit(data)
                self.scaler_initialized = True
                # بافر را کوچک نگه می‌داریم (نصف آخر)
                self.robust_buffer = self.robust_buffer[-ROBUST_BUFFER_MAX_ROWS // 2:]

            if self.scaler is not None and self.scaler_initialized:
                return self.scaler.transform(X_2d)
            else:
                # تا قبل از fit کامل، داده را بدون اسکیلینگ عبور می‌دهیم
                return X_2d

        else:
            # بدون اسکیلینگ
            return X_2d

    def _scale_for_test(self, X_2d: np.ndarray) -> np.ndarray:
        """
        اسکیلینگ در فاز TEST.
        - Standard / Robust: فقط transform با پارامترهای یادگرفته شده
        - Normalize: هر بار L2 روی هر ردیف
        """
        if self.scaler_type == "standard":
            if self.scaler is not None and self.scaler_initialized:
                return self.scaler.transform(X_2d)
            else:
                return X_2d

        elif self.scaler_type == "normalize":
            return self._l2_normalize(X_2d)

        elif self.scaler_type == "robust":
            if self.scaler is not None and self.scaler_initialized:
                return self.scaler.transform(X_2d)
            else:
                return X_2d

        else:
            return X_2d

    # ---------- لاگ متریک‌ها ----------

    def _init_metrics_csv(self) -> None:
        if not METRICS_CSV_PATH.exists():
            with METRICS_CSV_PATH.open("w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow([
                    "timestamp",
                    "phase",
                    "slot_id",
                    "row_index",
                    "label",
                    "prediction",
                    "correct",
                    "cumulative_accuracy"
                ])

    def _append_metrics(
        self,
        phase: str,
        slot_id: int,
        row_index: int,
        label: Optional[int],
        prediction: Optional[int]
    ) -> None:
        correct_flag = None
        if label is not None and prediction is not None:
            self.test_total += 1
            if prediction == label:
                self.test_correct += 1
            correct_flag = int(prediction == label)
            acc = self.test_correct / self.test_total
        else:
            acc = self.test_correct / self.test_total if self.test_total > 0 else 0.0

        with METRICS_CSV_PATH.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow([
                time.time(),
                phase,
                slot_id,
                row_index,
                label,
                prediction,
                correct_flag,
                acc
            ])

    # ---------- اینترفیس اصلی ----------

    def process_row(self, slot_id: int, X_row: np.ndarray, label: Optional[int]) -> None:
        # پیش‌پردازش (clean + سایر مراحل)
        X_pre = self.preproc.transform(X_row)
        X_2d = X_pre.reshape(1, -1)

        if self.mode == "TRAIN":
            self._train_step(slot_id, X_2d, label)
        else:
            self._test_step(slot_id, X_2d, label)

    def _train_step(self, slot_id: int, X_2d: np.ndarray, label: Optional[int]) -> None:
        if label is None:
            logging.warning("در TRAIN هستیم ولی label نداریم؛ فقط اسکیلینگ (در صورت نیاز) را warm-up می‌کنیم.")
            X_in = self._scale_for_train(X_2d)
            if self.model_initialized:
                y_pred = self.clf.predict(X_in)[0]
                logging.info(f"[TRAIN/no-label] Slot={slot_id} Pred={y_pred}")
            return

        # ۱) اسکیلینگ در TRAIN
        X_in = self._scale_for_train(X_2d)

        # ۲) partial_fit مدل
        y_arr = np.array([label])
        if not self.model_initialized:
            self.clf.partial_fit(X_in, y_arr, classes=CLASSES)
            self.model_initialized = True
            logging.info("✅ Model initialized with first partial_fit.")
        else:
            self.clf.partial_fit(X_in, y_arr)

        # ۳) پیش‌بینی و متریک
        y_pred = self.clf.predict(X_in)[0]
        self.train_rows_seen += 1
        logging.info(
            f"[TRAIN] row #{self.train_rows_seen} | Slot={slot_id} "
            f"true={label}, pred={y_pred}"
        )

        if self.train_rows_seen % METRICS_LOG_EVERY == 0:
            self._append_metrics(
                phase="TRAIN",
                slot_id=slot_id,
                row_index=self.train_rows_seen,
                label=label,
                prediction=y_pred,
            )

        # سوییچ اتوماتیک (اگر نخواهی، می‌توانی این قسمت را کامنت کنی)
        if self.train_rows_seen >= TRAIN_ROW_LIMIT:
            self.mode = "TEST"
            logging.info(
                f"*** TRAIN phase finished after {self.train_rows_seen} rows. "
                f"Switched to TEST mode (auto). ***"
            )

    def _test_step(self, slot_id: int, X_2d: np.ndarray, label: Optional[int]) -> None:
        if not self.model_initialized:
            logging.warning("Model not initialized yet; cannot predict.")
            return

        # اسکیلینگ در TEST
        X_in = self._scale_for_test(X_2d)

        y_pred = self.clf.predict(X_in)[0]

        logging.info(f"[TEST] Slot={slot_id} pred={y_pred}, true_label={label}")

        self._append_metrics(
            phase="TEST",
            slot_id=slot_id,
            row_index=self.train_rows_seen + self.test_total + 1,
            label=label,
            prediction=y_pred,
        )

    # ---------- کنترل مود از بیرون ----------

    def set_mode(self, new_mode: str, reason: str = "") -> None:
        new_mode = new_mode.upper()
        if new_mode not in ("TRAIN", "TEST"):
            logging.warning(f"درخواست مود نامعتبر: {new_mode}")
            return
        self.mode = new_mode
        logging.info(f"### Mode changed to {self.mode} ({reason}) ###")

# =============================
# مدیریت slotها
# =============================

slots: Dict[int, RowBuffer] = {}

def compute_slot_id(timestamp: float) -> int:
    return int(timestamp // SLOT_SIZE_SECONDS)

def get_or_create_row_buffer(slot_id: int) -> RowBuffer:
    if slot_id not in slots:
        slots[slot_id] = RowBuffer(slot_id)
    return slots[slot_id]

def remove_old_slots(current_slot_id: int, max_age_slots: int = 10) -> None:
    to_delete = [sid for sid in slots if sid < current_slot_id - max_age_slots]
    for sid in to_delete:
        del slots[sid]

# =============================
# MQTT callbacks
# =============================

model_manager = OnlineModelManager(MODEL_CONFIG)

def on_connect(client, userdata, flags, rc):
    if rc == 0:
        logging.info("Connected to MQTT broker.")
    else:
        logging.error(f"Failed to connect to MQTT broker. rc={rc}")

    client.subscribe(MQTT_SENSOR_TOPIC)
    logging.info(f"Subscribed to sensors topic: {MQTT_SENSOR_TOPIC}")

    client.subscribe(MQTT_CONTROL_TOPIC)
    logging.info(f"Subscribed to control topic: {MQTT_CONTROL_TOPIC}")

def on_message(client, userdata, msg):
    try:
        payload = msg.payload.decode("utf-8")

        # ۱) کنترل MODE
        if msg.topic == MQTT_CONTROL_TOPIC:
            mode_cmd = payload.strip().upper()
            if mode_cmd in ("TRAIN", "TEST"):
                model_manager.set_mode(mode_cmd, reason="by MQTT control/mode")
            else:
                logging.warning(f"Unknown control command on {MQTT_CONTROL_TOPIC}: {payload}")
            return

        # ۲) داده سنسورها
        data = json.loads(payload)

        device_id = data.get("device_id")
        if device_id is None:
            logging.warning("پیام بدون device_id دریافت شد؛ نادیده گرفته می‌شود.")
            return

        ts = float(data.get("timestamp", time.time()))
        slot_id = compute_slot_id(ts)

        row_buffer = get_or_create_row_buffer(slot_id)
        row_buffer.add_message(device_id, data)

        remove_old_slots(slot_id)

        if row_buffer.is_complete():
            X_row, label = row_buffer.build_row()
            logging.info(
                f"[Slot {slot_id}] NEW ROW | mode={model_manager.mode} | "
                f"X={X_row}, label={label}"
            )

            model_manager.process_row(slot_id, X_row, label)

            if slot_id in slots:
                del slots[slot_id]

    except Exception as e:
        logging.exception(f"Error in on_message: {e}")

# =============================
# main
# =============================

def main():
    client = mqtt.Client()
    client.on_connect = on_connect
    client.on_message = on_message

    logging.info("Connecting to MQTT broker...")
    client.connect(MQTT_BROKER, MQTT_PORT, keepalive=60)

    logging.info("MQTT client started. Waiting for messages...")
    client.loop_forever()

if __name__ == "__main__":
    main()




